{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2R_WD09bz8C"
   },
   "source": [
    "# CS 584 Assignment 5 -- Dependency Parsing\n",
    "\n",
    "#### Name: (Zhiyao Wen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmpBhfpib-XF"
   },
   "source": [
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. implementing a neural-network based dependency parser with the goal of maximizing performance on the UAS (Unlabeled Attachment Score) metric\n",
    "\n",
    "In this assignment, we will be using PyTorch with the CUDA option set to None.\n",
    "\n",
    "*** Please read the code and comments very carefully and install these packages (NumPy, sklearn, and tqdm) before you start ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6416,
     "status": "ok",
     "timestamp": 1636949170270,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "CSHIItNVcIeH",
    "outputId": "fb952a7e-31dd-45db-c8ae-3b739d9ff6ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in c:\\users\\10617\\anaconda3\\lib\\site-packages (1.10.1+cu113)\n",
      "Requirement already satisfied: torchvision in c:\\users\\10617\\anaconda3\\lib\\site-packages (0.11.2+cu113)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\10617\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\10617\\anaconda3\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\10617\\anaconda3\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: numpy in c:\\users\\10617\\anaconda3\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\10617\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\10617\\anaconda3\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\10617\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\10617\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\10617\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\10617\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install numpy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pv95c9Wc-GR"
   },
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between head words, and words which modify those heads. There are multiple types of dependency parsers, including transition-based parsers, graph-based parsers, and feature-based parsers. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:\n",
    "* A *stack* of words that are currently being processed\n",
    "* A *buffer* of words yet to be processed.\n",
    "* A list of *dependencies* predicted by the parser.\n",
    "\n",
    "\n",
    "Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words of the sentence in order. At each step, the parser applies a transition to the partial parse until its buffer is empty and the stack size is 1. The following transitions can be applied:\n",
    "* **SHIFT**: removes the first word from the buffer and pushes it onto the stack.\n",
    "* **LEFT-ARC**: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack, adding a first word → second word dependency to the dependency list.\n",
    "* **RIGHT-ARC**: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack, adding a second word → first word dependency to the dependency list.\n",
    "\n",
    "On each step, your parser will decide among the three transitions using a neural network classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf7B_vlmdx1Y"
   },
   "source": [
    "## 1. Transition Mechanics (60 points)\n",
    "In this section, you need to implement the transition mechanics your parser will use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 and 1.2 are written questions, please check the pdf handout for the details. (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EddMIWhSjytm"
   },
   "source": [
    "### 1.3 Parsing (Fill in the code, 20 points)\n",
    "\n",
    "There are two functions\n",
    "1. \\_\\_init\\_\\_() (10 points)\n",
    "2. parse_step() (10 points)\n",
    "\n",
    "Please follow the comments and fill in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1636949170271,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "NN9ZNcnReHmG"
   },
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "\n",
    "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
    "                                        Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes. Do NOT alter it in your code.\n",
    "        self.sentence = sentence\n",
    "\n",
    "        ### Start your code\n",
    "        ### Your code should initialize the following fields:\n",
    "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
    "        ###                 last element of the list.\n",
    "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
    "        ###                  buffer as the first item of the list\n",
    "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "        ###             tuples where each tuple is of the form (head, dependent).\n",
    "        ###             Order for this list doesn't matter.\n",
    "        ###\n",
    "        ### Note: The root token should be represented with the string \"ROOT\"\n",
    "        ### Note: If you need to use the sentence object to initialize anything, make sure to not directly \n",
    "        ###       reference the sentence object.  That is, remember to NOT modify the sentence object. \n",
    "\n",
    "        self.stack = ['ROOT']\n",
    "        self.buffer = sentence[:]\n",
    "        self.dependencies = []\n",
    "\n",
    "        ### End\n",
    "\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
    "                                left-arc, and right-arc transitions. You can assume the provided\n",
    "                                transition is a legal transition.\n",
    "        \"\"\"\n",
    "        ### Start your code\n",
    "        ### TODO:\n",
    "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
    "        ###     described above:\n",
    "        ###         1. Shift\n",
    "        ###         2. Left Arc\n",
    "        ###         3. Right Arc\n",
    "        if transition == \"S\":\n",
    "            self.stack.append(self.buffer[0])\n",
    "            self.buffer.pop(0)\n",
    "        elif transition == \"RA\":\n",
    "            self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
    "            self.stack.pop(-1)\n",
    "        else:\n",
    "            self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
    "            self.stack.pop(-2)\n",
    "\n",
    "        ### End\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
    "\n",
    "        @return dependencies (list of string tuples): The list of dependencies produced when\n",
    "                                                        parsing the sentence. Represented as a list of\n",
    "                                                        tuples where each tuple is of the form (head, dependent).\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi_bM9cOfuP7"
   },
   "source": [
    "Execute the following cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1636949170271,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "aNzqAqc3f3Nl",
    "outputId": "b03af80b-4e8f-441d-b015-355b5842f583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIFT test passed!\n",
      "LEFT-ARC test passed!\n",
      "RIGHT-ARC test passed!\n",
      "parse test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_step(name, transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    pp = PartialParse([])\n",
    "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
    "\n",
    "    pp.parse_step(transition)\n",
    "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
    "    assert stack == ex_stack, \\\n",
    "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
    "    assert buf == ex_buf, \\\n",
    "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "    print(\"{:} test passed!\".format(name))\n",
    "\n",
    "def test_parse_step():\n",
    "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
    "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
    "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
    "    \n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
    "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = tuple(sorted(dependencies))\n",
    "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
    "    assert dependencies == expected,  \\\n",
    "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
    "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
    "        \"parse test failed: the input sentence should not be modified\"\n",
    "    print(\"parse test passed!\")\n",
    "\n",
    "test_parse_step()\n",
    "test_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMHunGyxf-lm"
   },
   "source": [
    "### 1.4 Minibatch Dependecy Parsing (Fill in the code, 20 points)\n",
    "\n",
    "Since neural networks run much more efficiently when making predictions about batches of data at a time, in this section, you need to implement this Minibatch algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1636949170458,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "4qikoL9AhPwO"
   },
   "outputs": [],
   "source": [
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "\n",
    "    @param sentences (list of list of str): A list of sentences to be parsed\n",
    "                                            (each sentence is a list of words and each word is of type string)\n",
    "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
    "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "                                returns a list of transitions predicted for each parse. That is, after calling\n",
    "                                    transitions = model.predict(partial_parses)\n",
    "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
    "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
    "\n",
    "\n",
    "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
    "                                                    list for a parsed sentence. Ordering should be the\n",
    "                                                    same as in sentences (i.e., dependencies[i] should\n",
    "                                                    contain the parse for sentences[i]).\n",
    "    \"\"\"\n",
    "    dependencies = []\n",
    "\n",
    "    ### Start to code\n",
    "    ### TODO:\n",
    "    ###     Implement the minibatch parse algorithm.\n",
    "    ###\n",
    "    ###     Note: A shallow copy can be made with the \"=\" sign in python, e.g.\n",
    "    ###                 unfinished_parses = partial_parses[:].\n",
    "    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
    "    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n",
    "    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n",
    "    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n",
    "    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n",
    "    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n",
    "    ###             is being accessed by `partial_parses` and may cause your code to crash.\n",
    "\n",
    "    partial_parses = [PartialParse(s) for s in sentences]\n",
    "\n",
    "    unfinished_parse = partial_parses\n",
    "\n",
    "    while len(unfinished_parse) > 0:\n",
    "        minibatch = unfinished_parse[0:batch_size]\n",
    "        # perform transition and single step parser on the minibatch\n",
    "        while len(minibatch) > 0:\n",
    "            transitions = model.predict(minibatch)\n",
    "            for index, action in enumerate(transitions):\n",
    "                minibatch[index].parse_step(action)\n",
    "            minibatch = [parse for parse in minibatch if len(parse.stack) > 1 or len(parse.buffer) > 0]\n",
    "\n",
    "        # move to the next batch\n",
    "        unfinished_parse = unfinished_parse[batch_size:]\n",
    "\n",
    "    dependencies = []\n",
    "    for n in range(len(sentences)):\n",
    "        dependencies.append(partial_parses[n].dependencies)\n",
    "    ### End\n",
    "\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E9mhBJ3hxDl"
   },
   "source": [
    "Run the following cell to test your implementation.\n",
    "\n",
    "**Note:** You will need minibatch parse to be correctly implemented to evaluate the model you will build in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1636949170458,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "QJHg3KAnh9la",
    "outputId": "7d271f93-3489-4ed3-b288-8fedb75166c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch_parse test passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\10617\\AppData\\Local\\Temp/ipykernel_10652/644800228.py:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n"
     ]
    }
   ],
   "source": [
    "class DummyModel(object):\n",
    "    \"\"\"Dummy model for testing the minibatch_parse function\n",
    "    \"\"\"\n",
    "    def __init__(self, mode = \"unidirectional\"):\n",
    "        self.mode = mode\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        if self.mode == \"unidirectional\":\n",
    "            return self.unidirectional_predict(partial_parses)\n",
    "        elif self.mode == \"interleave\":\n",
    "            return self.interleave_predict(partial_parses)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def unidirectional_predict(self, partial_parses):\n",
    "        \"\"\"First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
    "        the sentence is \"right\", \"left\" if otherwise.\n",
    "        \"\"\"\n",
    "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]\n",
    "\n",
    "    def interleave_predict(self, partial_parses):\n",
    "        \"\"\"First shifts everything onto the stack and then interleaves \"right\" and \"left\".\n",
    "        \"\"\"\n",
    "        return [(\"RA\" if len(pp.stack) % 2 == 0 else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]\n",
    "\n",
    "def test_dependencies(name, deps, ex_deps):\n",
    "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
    "    deps = tuple(sorted(deps))\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "\n",
    "\n",
    "def test_minibatch_parse():\n",
    "    \"\"\"Simple tests for the minibatch_parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "\n",
    "    # Unidirectional arcs test\n",
    "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "                 [\"left\", \"arcs\", \"only\"],\n",
    "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
    "    \n",
    "    test_dependencies(\"minibatch_parse\", deps[0],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[1],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[2],\n",
    "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[3],\n",
    "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
    "\n",
    "    # Out-of-bound test\n",
    "    sentences = [[\"right\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
    "    test_dependencies(\"minibatch_parse\", deps[0], (('ROOT', 'right'),))\n",
    "\n",
    "    # Mixed arcs test\n",
    "    sentences = [[\"this\", \"is\", \"interleaving\", \"dependency\", \"test\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(mode=\"interleave\"), 1)\n",
    "    test_dependencies(\"minibatch_parse\", deps[0],\n",
    "                      (('ROOT', 'is'), ('dependency', 'interleaving'),\n",
    "                      ('dependency', 'test'), ('is', 'dependency'), ('is', 'this')))\n",
    "    print(\"minibatch_parse test passed!\")\n",
    "\n",
    "test_minibatch_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_JqUE-NkOM8"
   },
   "source": [
    "## 2. Neural Networks for parsing (40 points)\n",
    "\n",
    "In this section, you are going to build and train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1636949171378,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "DyfzMmYVvJgx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8HyxtLHkcuZ"
   },
   "source": [
    "## 2.1 Create your model (fill the code, 20 points)\n",
    "The input of model is a list of integers $\\mathbf{w} =[w_1, w_2, ..., w_m]$ where $m$ is the number of features. Then our network looks up an embedding for each word and concatenates them into a single input vector:\n",
    "\n",
    "$\\mathbf{x} = [\\mathbf{E}_{w_1}, \\mathbf{E}_{w_2}, ..., \\mathbf{E}_{w_m}]$\n",
    "\n",
    "where $\\mathbf{E}\\in \\mathbb{R}^{|V|\\times d}$ is an embedding matrix with each row $\\mathbf{E}_w$ as the vector for a particular word $w$.\n",
    "\n",
    "Then, we compuate our prediction as:\n",
    "\n",
    "> $\\mathbf{h} = ReLU(xW + b_1)$\n",
    "\n",
    "> $\\mathbf{l} = hU + b_2$\n",
    "\n",
    "> $\\hat{y} = softmax(l)$\n",
    "\n",
    "where $\\mathbf{h}$ is referred to as the hidden layer, $\\mathbf{l}$ is referred to as the logits, $\\hat{y}$ is referred to as the predictions, and $ReLU(z)=max(z, 0)$.\n",
    "\n",
    "We will train the model to minimize cross-entropty loss:\n",
    "\n",
    "> $J(\\theta)=CE(y, \\hat{y})=-\\sum_{i=1}^3 y_i \\log \\hat{y}_i$\n",
    "\n",
    "To compute the loss for the training set, we average this J(θ) across all training examples.\n",
    "We will use UAS score as our evaluation metric. UAS refers to Unlabeled Attachment Score, which is computed as the ratio between number of correctly predicted dependencies and the number of total dependencies despite of the relations (our model doesn’t predict this).\n",
    "\n",
    "**Note:**\n",
    "To test your understanding of embedding lookup, so **DO NOT** use any high level API like **nn.Embedding** and **nn.Linear** in your code, otherwise you will receive deductions.\n",
    "\n",
    "**Hints:**\n",
    "* Each of the variables you are asked to declare (self.embed to hidden weight, self.embed to hidden bias, self.hidden to logits weight, self.hidden to logits bias) corresponds to one of the variables above (W, b1, U, b2).\n",
    "* It should take about 1 hour to train the model on the entire the training dataset, i.e., when debug mode is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1636949171379,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "SAVwU01ulIjU"
   },
   "outputs": [],
   "source": [
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and two hidden layers.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "\n",
    "        @param embeddings (ndarray): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        ### Start your code\n",
    "        ### TODO:\n",
    "        ###     1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `Parann.meter`.\n",
    "        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n",
    "        ###        with default parameters.\n",
    "        ###     2) Construct `self.dropout` layer.\n",
    "        ###     3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`.\n",
    "        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n",
    "        ###        with default parameters.\n",
    "        ###\n",
    "        ### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API\n",
    "        ###       to include a tensor into a computational graph to support updating w.r.t its gradient.\n",
    "        ###       Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
    "        ###       It has been shown empirically, that this provides better initial weights\n",
    "        ###       for training networks than random uniform initialization.\n",
    "        ###       For more details checkout this great blogpost:\n",
    "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters\n",
    "        ###     Initialization: https://pytorch.org/docs/stable/nn.init.html\n",
    "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers\n",
    "        ### \n",
    "        \n",
    "        #(1)\n",
    "        self.embed_to_hidden_weight =nn.init.xavier_uniform_(nn.Parameter(torch.empty(n_features * self.embed_size, hidden_size)))\n",
    "        \n",
    "        \n",
    "        self.embed_to_hidden_bias = nn.init.uniform_(nn.Parameter(torch.empty(hidden_size)))\n",
    "       \n",
    "        \n",
    "        \n",
    "        #(2)\n",
    "        self.dropout = nn.Dropout(p = self.dropout_prob)\n",
    "        \n",
    "        #(3)\n",
    "        self.hidden_to_logits_weight = nn.init.xavier_uniform_(nn.Parameter(torch.empty(hidden_size,self.n_classes)))\n",
    "        \n",
    "        \n",
    "        self.hidden_to_logits_bias =  nn.init.uniform_(nn.Parameter(torch.empty(self.n_classes))) \n",
    "       \n",
    "        ### End\n",
    "\n",
    "    def embedding_lookup(self, w):\n",
    "        \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings`\n",
    "            @param w (Tensor): input tensor of word indices (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in w\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "\n",
    "        ### Start your code\n",
    "        ### TODO:\n",
    "        ###     1) For each index `i` in `w`, select `i`th vector from self.embeddings\n",
    "        ###     2) Reshape the tensor using `view` necessaryfunction if \n",
    "        ###\n",
    "        ### Note: All embedding vectors are stacked and stored as a matrix. The model receives\n",
    "        ###       a list of indices representing a sequence of words, then it calls this lookup\n",
    "        ###       function to map indices to sequence of embeddings.\n",
    "        ###\n",
    "        ###       This problem aims to test your understanding of embedding lookup,\n",
    "        ###       so DO NOT use any high level API like nn.Embedding\n",
    "        ###       (we are asking you to implement that!). Pay attention to tensor shapes\n",
    "        ###       and reshape if necessary. Make sure you know each tensor's shape before you run the code!\n",
    "        ###\n",
    "        ### Pytorch has some useful APIs for you, and you can use either one\n",
    "        ### in this problem (except nn.Embedding). These docs might be helpful:\n",
    "        ###     Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select\n",
    "        ###     Gather: https://pytorch.org/docs/stable/torch.html#torch.gather\n",
    "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        ###     Flatten: https://pytorch.org/docs/stable/generated/torch.flatten.html\n",
    "\n",
    "        x = torch.index_select(self.embeddings, 0, w.reshape(1,-1)[0])\n",
    "        x = x.view(w.shape[0], -1)\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "       \n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, w):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,\n",
    "                    the `forward` function would called on `w` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(w) # this calls the forward function\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param w (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### Start your code\n",
    "        ### TODO:\n",
    "        ###     Complete the forward computation as described in write-up. In addition, include a dropout layer\n",
    "        ###     as decleared in `__init__` after ReLU function.\n",
    "        ###\n",
    "        ### Note: We do not apply the softmax to the logits here, because\n",
    "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul\n",
    "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
    "        \n",
    "        x = self.embedding_lookup(w)\n",
    "        h = F.relu(torch.matmul(x, self.embed_to_hidden_weight) + self.embed_to_hidden_bias)\n",
    "        logits = torch.matmul(h, self.hidden_to_logits_weight) + self.hidden_to_logits_bias\n",
    "\n",
    "        ### End\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follwoing cell to test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "error",
     "timestamp": 1636949171657,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "bqOLP639m_kA",
    "outputId": "5029a250-4ba2-49bb-f9e6-513cf48dcbce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_lookup sanity check passes!\n",
      "Forward sanity check passes!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = np.zeros((100, 30), dtype=np.float32)\n",
    "model = ParserModel(embeddings)\n",
    "\n",
    "def check_embedding():\n",
    "    inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "    selected = model.embedding_lookup(inds)\n",
    "    assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n",
    "                                    + repr(selected) + \" contains non-zero elements.\"\n",
    "\n",
    "def check_forward():\n",
    "    inputs =torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "    out = model(inputs)\n",
    "    expected_out_shape = (4, 3)\n",
    "    assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n",
    "                                            \" which doesn't match expected \" + repr(expected_out_shape)\n",
    "\n",
    "check_embedding()\n",
    "print(\"Embedding_lookup sanity check passes!\")\n",
    "\n",
    "check_forward()\n",
    "print(\"Forward sanity check passes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnTa0evHldfT"
   },
   "source": [
    "### 2.2 Training (Fill the code, 20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1636949171656,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "QkAHYd3_le93"
   },
   "outputs": [],
   "source": [
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
    "    \"\"\" Train the neural dependency parser.\n",
    "\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param output_path (str): Path to which model weights and results are written.\n",
    "    @param batch_size (int): Number of examples in a single batch\n",
    "    @param n_epochs (int): Number of training epochs\n",
    "    @param lr (float): Learning rate\n",
    "    \"\"\"\n",
    "    best_dev_UAS = 0\n",
    "\n",
    "\n",
    "    ### Start your code\n",
    "    ### TODO:\n",
    "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
    "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`\n",
    "    ###         reduction (default)\n",
    "    ###\n",
    "    ### Hint: Use `parser.model.parameters()` to pass optimizer\n",
    "    ###       necessary parameters to tune.\n",
    "    ### Please see the following docs for support:\n",
    "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
    "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "    loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    ### END\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
    "    \"\"\" Train the neural dependency parser for single epoch.\n",
    "\n",
    "    Note: In PyTorch we can signify train versus test and automatically have\n",
    "    the Dropout Layer applied and removed, accordingly, by specifying\n",
    "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
    "\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
    "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
    "    @param batch_size (int): batch size\n",
    "\n",
    "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
    "    \"\"\"\n",
    "    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
    "            optimizer.zero_grad()   # remove any baggage in the optimizer\n",
    "            loss = 0. # store loss for this batch here\n",
    "            train_x = torch.from_numpy(train_x).long()\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
    "\n",
    "            ### Start your code\n",
    "            ### TODO:\n",
    "            ###      1) Run train_x forward through model to produce `logits`\n",
    "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
    "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
    "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
    "            ###         are the predictions (y^ from the PDF).\n",
    "            ###      3) Backprop losses\n",
    "            ###      4) Take step with the optimizer\n",
    "            ### Please see the following docs for support:\n",
    "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
    "            \n",
    "            logits = model(train_x)\n",
    "            loss = loss_func(logits, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            ### End\n",
    "            \n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1636949171656,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "jj_KcipUl5X1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 1.12 seconds\n",
      "Building parser...\n",
      "took 0.79 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 1.92 seconds\n",
      "Vectorizing data...\n",
      "took 0.85 seconds\n",
      "Preprocessing training data...\n",
      "took 29.86 seconds\n",
      "took 0.01 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17933be36ef64453b9110ba44bdfc513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1535139179373265\n",
      "Evaluating on dev set\n",
      "- dev UAS: 84.78\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29ee989a11841a086f1f039bf7347c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.08936800102570228\n",
      "Evaluating on dev set\n",
      "- dev UAS: 86.76\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffd321d810a4323ba33f0c8a66989e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.0748340087837955\n",
      "Evaluating on dev set\n",
      "- dev UAS: 87.25\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcaa674f3704dd7b6f182b1635c6182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.0652714147659833\n",
      "Evaluating on dev set\n",
      "- dev UAS: 88.10\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19762c1ac1f4be3ba5a5a549868c7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.05741772959936104\n",
      "Evaluating on dev set\n",
      "- dev UAS: 88.85\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dcfde8261548789a1dceb148059158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.05080009218672234\n",
      "Evaluating on dev set\n",
      "- dev UAS: 88.41\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c404b3e3e845cd86fea88c5bcb7ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.044936041697432406\n",
      "Evaluating on dev set\n",
      "- dev UAS: 88.84\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88973dd83e5e49039f960b7fb656d7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.03941662022057776\n",
      "Evaluating on dev set\n",
      "- dev UAS: 88.90\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c935cd00ea4c55b1af539a3c776eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.03433534057581096\n",
      "Evaluating on dev set\n",
      "- dev UAS: 88.66\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933ce8af4992419489221367176027b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n",
    "import torch.optim as optim \n",
    "\n",
    "debug = False ## Set to True if you want to debug your code\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"INITIALIZING\")\n",
    "print(80 * \"=\")\n",
    "parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(minibatch_parse, debug)\n",
    "\n",
    "start = time.time()\n",
    "model = ParserModel(embeddings)\n",
    "parser.model = model\n",
    "print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "if not debug:\n",
    "    print(80 * \"=\")\n",
    "    print(\"TESTING\")\n",
    "    print(80 * \"=\")\n",
    "    print(\"Restoring the best model weights found on the dev set\")\n",
    "    parser.model.load_state_dict(torch.load(output_path))\n",
    "    print(\"Final evaluation on test set\",)\n",
    "    parser.model.eval()\n",
    "    UAS, dependencies = parser.parse(test_data)\n",
    "    print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1636949171657,
     "user": {
      "displayName": "Kun Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOzS82rlRKbzvZc0-xvBlrFtJvmVIA4XlvXJk2=s64",
      "userId": "07404482494479019566"
     },
     "user_tz": 300
    },
    "id": "99ifgGzamM6c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6zEZ+Swvk1/Vg9PZKb5Rm",
   "collapsed_sections": [],
   "name": "CS584A_21F_Assignment5_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
