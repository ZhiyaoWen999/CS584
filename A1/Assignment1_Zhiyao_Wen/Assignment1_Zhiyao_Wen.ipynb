{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae71131",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 1 -- Text Classification (Machine Learning and NLP Basics)\n",
    "\n",
    "#### Name: (Zhiyao Wen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7ad9f7",
   "metadata": {},
   "source": [
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the preprocessing.\n",
    "3. Implement tokenization.\n",
    "4. Implement feature extraction.\n",
    "5. Implement Logistic Regression.\n",
    "6. Implement Stochastic Gradient Descent and Mini-batch Gradient Descent.\n",
    "7. Evaluate all the experiments and compare all the results.\n",
    "\n",
    "*** Please read the code very carefully and install these packages (NumPy, Pandas, sklearn, tqdm, and matplotlib) before you start ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa03677",
   "metadata": {},
   "source": [
    "## 1. Data Processing (30 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data by using Pandas\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620121be",
   "metadata": {},
   "source": [
    "### 1.1 Load Data\n",
    "\n",
    "Run the following cells (Please make sure the paths of data files are correct.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c280da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                text  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv', header=None)\n",
    "train_df.columns = ['label', 'title', 'text']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4e6e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8ba7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      3                  Fears for T N pension after talks   \n",
       "1      4  The Race is On: Second Private Team Sets Launc...   \n",
       "2      4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3      4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4      4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "\n",
       "                                                text  \n",
       "0  Unions representing workers at Turner   Newall...  \n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...  \n",
       "2  AP - A company founded by a chemistry research...  \n",
       "3  AP - It's barely dawn when Mike Fitzpatrick st...  \n",
       "4  AP - Southern California's smog-fighting agenc...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./data/test.csv', header=None)\n",
    "test_df.columns = ['label', 'title', 'text']\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66ac6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658f091",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess (Fill the code: 10 points)\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad27419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "class Preprocesser(object):\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "    \n",
    "    def apply(self, text):\n",
    "        \n",
    "        text = self._lowercase(text)\n",
    "        \n",
    "        if self.url:\n",
    "            text = self._remove_url(text)\n",
    "            \n",
    "        if self.punctuation:\n",
    "            text = self._remove_punctuation(text)\n",
    "            \n",
    "        if self.number:\n",
    "            text = self._remove_number(text)\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "        \n",
    "    def _remove_punctuation(self, text):\n",
    "        ''' Please fill this function to remove all the punctuations in the text\n",
    "        '''\n",
    "        ### Start your code\n",
    "        \n",
    "        text = re.sub(r'[^\\w\\s]', ' ',text) # remove punctuation with regular expression\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_url(self, text):\n",
    "        ''' Please fill this function to remove all the urls in the text\n",
    "        '''\n",
    "        ### Start your code\n",
    "        \n",
    "        text = re.sub(r'http\\S+', '', text) # remove url with regular expression\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_number(self, text):\n",
    "        ''' Please fill this function to remove all the numbers in the text\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        text = re.sub(\"(\\s\\d+)\",\"\",text) # remove number in text with regular expression\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _lowercase(self, text):\n",
    "        ''' Please fill this function to lowercase the text\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        text = text.lower() # change text char to lower case\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e92ed",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e258516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets \"\n"
     ]
    }
   ],
   "source": [
    "text = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processer = Preprocesser()\n",
    "clean_text = processer.apply(text)\n",
    "\n",
    "print(f'\"{text}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_text}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082cb324",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization (Fill the code: 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db45f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(text):\n",
    "    ''' Please fill this function to tokenize text.\n",
    "            1. Tokenize the text.\n",
    "            2. Remove stop words.\n",
    "            3. Optional: lemmatize words accordingly.\n",
    "    '''\n",
    "    \n",
    "    ### Start your code\n",
    "    tokens = text.split()\n",
    "    \n",
    "    stop_words = stopwords.words() # to remove stop words\n",
    "    \n",
    "    # Remove any nonalphabetic, stopwords,or single letter words\n",
    "    \n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words and len(token) > 1]\n",
    "    ### End\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb3e4f",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0793a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets. ==> ['interest', 'rates', 'trimmed', 'south', 'african', 'central', 'bank', 'lack', 'warning', 'hits', 'rand', 'surprises', 'markets']\n"
     ]
    }
   ],
   "source": [
    "text = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processer = Preprocesser()\n",
    "clean_text = processer.apply(text)\n",
    "tokens = tokenize(clean_text)\n",
    "\n",
    "print(f'{text} ==> {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1977c",
   "metadata": {},
   "source": [
    "### 1.4 Data split (Fill the code: 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea23fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set: 108000\n",
      "The size of validation set: 12000\n",
      "The size of test set: 7600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train = train_df['text'].values.astype(str)\n",
    "label_train = train_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "text_test = test_df['text'].values.astype(str)\n",
    "label_test = test_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "\n",
    "### Start your code, split the text_train and label_train into training and validation\n",
    "### Make sure the names of varables are \"text_train\", \"label_train\", \"text_valid\", and \"label_valid\"\n",
    "\n",
    "text_train, text_valid, label_train, label_valid = train_test_split(text_train,label_train, train_size = 0.9, random_state =42)\n",
    "\n",
    "\n",
    "### End\n",
    "\n",
    "print('The size of training set:', text_train.shape[0])\n",
    "print('The size of validation set:', text_valid.shape[0])\n",
    "print('The size of test set:', text_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60824f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb0f2069",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction (Fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92767ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "class TfIdfExtractor(object):\n",
    "    \n",
    "    def __init__(self, vocab_size=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.vocab = defaultdict(lambda: 0)\n",
    "        self.word2idx = {}\n",
    "        self.df = defaultdict(lambda: 0)\n",
    "        self.num_doc = 0\n",
    "        \n",
    "        self.processer = Preprocesser()\n",
    "        \n",
    "        \n",
    "    def fit(self, texts):\n",
    "        ''' In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary (self.vocab).\n",
    "                2. Construct the document frequency dictionary (self.df).\n",
    "                3. Sort the vocabulary based on the frequency (self.vocab).\n",
    "            Input:\n",
    "                texts: a list of text (training set)\n",
    "            Output:\n",
    "                None\n",
    "        '''\n",
    "\n",
    "        self.num_doc = len(texts)\n",
    "        \n",
    "        for text in tqdm(texts, desc='fitting text'):\n",
    "            clean_text = self.processer.apply(text)\n",
    "            tokens = tokenize(clean_text)\n",
    "            \n",
    "            \n",
    "            ### Start your code (step 1 & 2)\n",
    "            \n",
    "            for token in tokens:\n",
    "                self.vocab[token]+=1         #construct the vocabulary dictionary with the number of the word appear in the text\n",
    "                if token not in self.vocab:\n",
    "                    self.df[token]+=1        # construct the df that docement contains the certain word\n",
    "\n",
    "            \n",
    "            ### End\n",
    "            \n",
    "\n",
    "        ### Start your code (Step 3)\n",
    "    \n",
    "        self.vocab = sorted(self.vocab.items(), key=lambda x:x[1], reverse=True)   #sorted the vocab from large number to small number\n",
    "\n",
    "        # after sorted, self.vocab becomes a list, so chang it back to dic\n",
    "        self.vocab = {key:value for key, value in self.vocab}\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        if self.vocab_size is not None:\n",
    "            self.vocab = {key: self.vocab[key] for key in list(self.vocab.keys())[:self.vocab_size]}\n",
    "        \n",
    "        self.word2idx = {key: idx for idx, key in enumerate(self.vocab.keys())}\n",
    "\n",
    "\n",
    "    def transform(self, texts):\n",
    "        ''' In this function, you need to encode the input text into TF-IDF vector.\n",
    "            Input:\n",
    "                texts: a list of text.\n",
    "            Ouput:\n",
    "                a N-d matrix (Tf-Idf) \n",
    "        '''\n",
    "        tfidf = np.zeros((len(texts), len(self.vocab)))\n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), desc='transforming', total=len(texts)):\n",
    "            clean_text = self.processer.apply(text)\n",
    "            tokens = tokenize(clean_text)\n",
    "            \n",
    "            ### Start your code\n",
    "            for word, j in self.word2idx.items():\n",
    "                counts = tokens.count(word) # count the word in tokens\n",
    "                tfidf[i][j] = counts\n",
    "                \n",
    "        tf_idf = np.where(tfidf ==0,0,1)  # when tfidf =0, to find 0, else 1\n",
    "        idf = np.sum(tf_idf,axis = 0)  # sum idf\n",
    "        \n",
    "        \n",
    "        #         print(idf)\n",
    "        tfidf = tfidf * (1/idf) # fit to TF-IDF vector\n",
    "        \n",
    "        #         print(tfidf)\n",
    "            ### End\n",
    "        \n",
    "        return tfidf\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50cb401",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cbe7af2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9586c94751384938b89f5be99abdd82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fitting text:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81321237c2194b9b850844cdd9c566a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10617\\AppData\\Local\\Temp/ipykernel_16488/3151941048.py:91: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  tfidf = tfidf * (1/idf) # fit to TF-IDF vector\n",
      "C:\\Users\\10617\\AppData\\Local\\Temp/ipykernel_16488/3151941048.py:91: RuntimeWarning: invalid value encountered in multiply\n",
      "  tfidf = tfidf * (1/idf) # fit to TF-IDF vector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, 0. , 0. , 0. , 0. , 0. , nan, 0. , 0. , 0. ],\n",
       "       [nan, 0. , 0. , 0. , 0. , 0. , nan, 0. , 0. , 0. ],\n",
       "       [nan, 0. , 1. , 0. , 0. , 0. , nan, 0.5, 0. , 0. ],\n",
       "       [nan, 0. , 0. , 0. , 0. , 0. , nan, 0. , 0. , 0. ],\n",
       "       [nan, 0. , 0. , 0. , 0. , 1. , nan, 0. , 1. , 0. ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = TfIdfExtractor(vocab_size=10)\n",
    "extractor.fit(text_train[:100])\n",
    "X = extractor.transform(text_train[:10])\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66961667",
   "metadata": {},
   "source": [
    "#### 1.5.4 Run the following code to obtain the TD-IDF and One-hot labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42a9c1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a841aeee9b34a7a94095ed23ef0df55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fitting text:   0%|          | 0/108000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9575c895e36a40f4923d6b0f4bb8e529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/108000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f48c8a65e204b01946eabae7438e6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/12000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4494e51f94914511b8a5c399ebc6ba92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set: (108000, 4000)\n",
      "The size of validation set: (12000, 4000)\n",
      "The size of test set: (7600, 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10617\\AppData\\Local\\Temp/ipykernel_16488/3151941048.py:91: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  tfidf = tfidf * (1/idf) # fit to TF-IDF vector\n",
      "C:\\Users\\10617\\AppData\\Local\\Temp/ipykernel_16488/3151941048.py:91: RuntimeWarning: invalid value encountered in multiply\n",
      "  tfidf = tfidf * (1/idf) # fit to TF-IDF vector\n"
     ]
    }
   ],
   "source": [
    "# You can change this number to see the difference of the performances. (larger vocab size needs more memory)\n",
    "vocab_size = 4000 \n",
    "num_class = 4\n",
    "\n",
    "extractor = TfIdfExtractor(vocab_size=vocab_size)\n",
    "extractor.fit(text_train)\n",
    "\n",
    "x_train = extractor.transform(text_train)\n",
    "x_valid = extractor.transform(text_valid)\n",
    "x_test = extractor.transform(text_test)\n",
    "\n",
    "\n",
    "# convert label to one-hot vector\n",
    "y_train = np.zeros((label_train.shape[0], num_class))\n",
    "y_train[np.arange(label_train.shape[0]), label_train] = 1\n",
    "\n",
    "y_valid = np.zeros((label_valid.shape[0], num_class))\n",
    "y_valid[np.arange(label_valid.shape[0]), label_valid] = 1\n",
    "\n",
    "y_test = np.zeros((label_test.shape[0], num_class))\n",
    "y_test[np.arange(label_test.shape[0]), label_test] = 1\n",
    "\n",
    "\n",
    "print('The size of training set:', x_train.shape)\n",
    "print('The size of validation set:', x_valid.shape)\n",
    "print('The size of test set:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd3865",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression (60 points)\n",
    "In this section, you are required to implement a Logistic Regression(LR) model with $L_2$ regularization from scratch. \n",
    "\n",
    "\n",
    "The objective function of LR:\n",
    "\n",
    "<center> $J = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$ </center>\n",
    "\n",
    "**Deliverable 1**: Given the objective function, please show the steps to derive the graident of J with respecty of $w_k$. You can either list the steps in the notebook or submit a pdf with all the steps in the submission. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa4729",
   "metadata": {},
   "source": [
    "### 2.1 LR and softmax function (Fill the code, 20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4462a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ''' Compute the softmax function for each row of the input x.\n",
    "        \n",
    "        Inputs:\n",
    "            x: A D dimensional vector or N x D dimensional numpy matrix.\n",
    "        Outputs:\n",
    "            x: You are allowed to modify x in-place\n",
    "    '''\n",
    "    ### Start your code\n",
    "    \n",
    "\n",
    "    x = np.exp(x) / np.sum(np.exp(x),axis = 0)\n",
    "    ### End\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self, vocab_size, num_class, lam):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_class = num_class\n",
    "        self.lam = lam\n",
    "        \n",
    "        ### Start your code (initialize weight(w) and bias(b))\n",
    "        ### hint: you could use np.random.rand() to randomly initialize the parameters\n",
    "        \n",
    "        self.bias = np.random.rand((x.shape[0],1))  # how to know the x dimension ?\n",
    "        \n",
    "        x_bias = np.append(self.bias, x, axis=1)\n",
    "        \n",
    "        self.weight = np.random.rand(( 1, x_bias.shape[1]))\n",
    "        \n",
    "        \n",
    "        ### End\n",
    "        \n",
    "    def objective(self, x, y):\n",
    "        ''' Implement the objective function\n",
    "            Inputs:\n",
    "                x: N-d matrix\n",
    "                y: N-K matrix\n",
    "            Output: \n",
    "                the objective value of LR (scalar)\n",
    "        '''\n",
    "        loss = 0\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        fx = np.dot( self.weight,x) + self.bias\n",
    "        \n",
    "        fx = softmax(fx)\n",
    "        \n",
    "        n = y.shape\n",
    "        \n",
    "        # objective funtion\n",
    "        loss = -(1/n) * np.sum(y * np.log(fx)) + self.lam * (np.linalg.norm(self.weight) * np.linalg.norm(self.weight)) \n",
    "\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        ''' Implement the gradient of J with respect to w (in Deliverable 1)\n",
    "            Inputs:\n",
    "                x: N-d matrix\n",
    "                y: N-K matrix\n",
    "            Output:\n",
    "                w_grad: the gradient of J w.r.t weight\n",
    "                b_grad: the gradient of J w.r.t bias (K dimensional vector)\n",
    "        '''\n",
    "\n",
    "        n = x.shape[0]\n",
    "        d = x.shape[1]\n",
    "        K = y.shape[1]\n",
    "        \n",
    "        w_grad = 0. # 0 is just a placeholder, it should be a d-K matrix\n",
    "        b_grad = 0. # 0 is just a placeholder, it should be a K dimensional matrix\n",
    "        \n",
    "        w_grad = np.zeros((d,K))\n",
    "        b_grad = np.zeros((1,K))\n",
    "         \n",
    "        \n",
    "        # Compute Wx(for softmax denominator/numerator)\n",
    "        expons = np.exp(np.dot(self.weight,x))\n",
    "        \n",
    "        ### Start your code\n",
    "        for k in range(K):\n",
    "            \n",
    "            l2 = 2 * lam * self.weight[k] # for l2\n",
    "            \n",
    "            w_grad[k, :] = -self.lam*(((expons[k] / np.sum(expons, axis=0)) - y[:, k]).dot(x.T[:,k]) + l2)\n",
    "            \n",
    "            b_grad = -1/n * np.mean(y[:,k] * (np.ones((1,k))-softmax( expons  + self.bias)[:,k]))\n",
    "\n",
    "            \n",
    "        ### End\n",
    "        \n",
    "        return w_grad, b_grad\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, w_grad, b_grad, lr):     \n",
    "        ''' Implement the graident descent. \n",
    "            Updating weights and bias based on Equation: w = w - learning_rate * gradient)\n",
    "            \n",
    "            Inputs:\n",
    "                w_grad: a matrix which is the gradient of J w.r.t to weight\n",
    "                b_grad: a vector wich is the graident of J w.r.t to bias\n",
    "            Output:\n",
    "                None\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        K = self.weight.shape\n",
    "        \n",
    "        for k in range(K):\n",
    "            self.weight = self.weight - lr * w_grad # times lr\n",
    "            self.bias = self.bias - lr * b_grad\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_hat = softmax(np.dot(x, self.w)).squeeze()\n",
    "        return np.argmax(y_hat, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd4ca6",
   "metadata": {},
   "source": [
    "### 2.2 Stochastic Gradient Descent (SGD) (Fill the code, 15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3108b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model, X, y, lr, lam, num_epoch=100):\n",
    "    ''' Implement SGD\n",
    "        Inputs:\n",
    "            X: N-d matrix\n",
    "            y: N-K matrix\n",
    "            lr: learning rate\n",
    "            lam: lambda\n",
    "            num_epoch: the number of epochs\n",
    "        Output:\n",
    "            1. A list of training losses against epoch\n",
    "            2. A list of validation losses against epoch\n",
    "    '''\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        train_loss = 0.\n",
    "        \n",
    "        ### Start your code here (Please implement SGD and obtain the training loss)\n",
    "        \n",
    "        X, y = shuffle(X, y) # shuffle data\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            \n",
    "            ind = np.random.randint(0,len(X)) # select one random index\n",
    "            \n",
    "            X_sample = X[ind:ind+1] # random select one sample for each epoch\n",
    "            y_sample = y[ind:ind+1]\n",
    "            \n",
    "            # Compute x.TW (for softmax denominator/numerator)\n",
    "            expons = softmax(np.dot( model.weight,X_sample) + model.bias)\n",
    "            \n",
    "            \n",
    "            for k in range(y.shape[1]):\n",
    "                # Get l2 regularization term \n",
    "                l2 = 2 * lam * model.weight[k]\n",
    "                \n",
    "                train_loss = (train_loss -1 * np.sum(np.dot(y_sample,np.log(expons))) + l2) /n\n",
    "                \n",
    "                  \n",
    "            # Update w\n",
    "            w_grad, b_grad = nodel.gradient(X_sample, y_sample)\n",
    "            model.gradient_descent(w_grad, b_grad,lr)\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        valid_loss = 0.\n",
    "        \n",
    "        ### Start your code (Using validation set to obtain the validation loss)\n",
    "\n",
    "\n",
    "        valid_loss = model.objective(x_valid,y_valid)\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        \n",
    "        print(f'At epoch {e+1}, training loss: {train_loss:.4f}, validation loss: {valid_loss:.4f}.')\n",
    "        train_losses.append(np.mean(losses))\n",
    "        valid_losses.append(np.mean(losses))\n",
    "            \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ca037",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Update the hyper-parameters (num_epoch, lr, and lam) according to your observation to achieve better performance.\n",
    "'''\n",
    "num_epoch = 20\n",
    "lr = 0.001\n",
    "lam = 1E-6\n",
    "\n",
    "sgd_lr = LogisticRegression(vocab_size, num_class, lam)\n",
    "sgd_train_losses, sgd_valid_losses = sgd(sgd_lr, x_train, y_train, lr, lam, num_epoch)  \n",
    "\n",
    "\n",
    "# The class I filled may not be compiled, but it should show some ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b058d7",
   "metadata": {},
   "source": [
    "Run SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec5bbf",
   "metadata": {},
   "source": [
    "### 2.3 Mini-batch Gradient Descent (Fill the code: 15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e8b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gd(model, X, y, batch_size, lr, lam, num_epoch=100):\n",
    "    ''' Implement SGD\n",
    "        Inputs:\n",
    "            X: N-d matrix\n",
    "            y: N-K matrix\n",
    "            lr: learning rate\n",
    "            lam: lambda\n",
    "            num_epoch: the number of epochs\n",
    "        Output:\n",
    "            1. A list of training losses against epoch\n",
    "            2. A list of validation losses against epoch\n",
    "    '''\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        train_loss = 0.\n",
    "        \n",
    "        ### Start your code here (Implement Mini-batch GD)\n",
    "        \n",
    "        # the step is the same as SGD, except the select data, which is selecting small batch data for each epoch\n",
    "        X, y = shuffle(X, y)\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            \n",
    "            \n",
    "            \n",
    "            X_sample = X[:batch_size] # random select one sample \n",
    "            y_sample = y[:batch_size]\n",
    "            \n",
    "            # Compute x.TW (for softmax denominator/numerator)\n",
    "            expons = softmax(np.dot(model.weight,X_sample) + model.bias)\n",
    "            \n",
    "            \n",
    "            for k in range(y.shape[1]):\n",
    "                # Get l2 regularization term \n",
    "                l2 = 2 * lam * model.weight[k]\n",
    "                \n",
    "                train_loss = (train_loss -1 * np.sum(np.dot(y_sample,np.log(expons))) + l2) /n\n",
    "                \n",
    "                  \n",
    "            # Update w\n",
    "            w_grad, b_grad = nodel.gradient(X_sample, y_sample)\n",
    "            model.gradient_descent(w_grad, b_grad,lr)\n",
    "        \n",
    "            train_loss = train_loss /batch_size # divide the btach_size\n",
    "        ### End\n",
    "        \n",
    "        valid_loss = 0.\n",
    "        \n",
    "        ### Start your code (Using validation set to obtain the validation loss)\n",
    "\n",
    "        valid_loss = model.objective(x_valid,y_valid)\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(f'At epoch {e+1}, training loss: {train_loss:.4f}, validation loss: {valid_loss:.4f}.')\n",
    "            \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1153f62",
   "metadata": {},
   "source": [
    "Run Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Update the hyper-parameters (num_epoch, lr, lam, and batch_size) according to your observation \n",
    "    to achieve better performance.\n",
    "'''\n",
    "\n",
    "num_epoch = 20\n",
    "lr = 0.01\n",
    "lam = 1E-6\n",
    "batch_size = 32\n",
    "\n",
    "mini_gd_lr = LogisticRegression(vocab_size, num_class, lam)\n",
    "mini_gd_train_losses, mini_gd_valid_losses = mini_batch_gd(mini_gd_lr, x_train, y_train, batch_size, lr, lam, num_epoch)\n",
    "\n",
    "#The class I filled may not be compiled, but it should show some ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221a8ff",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation\n",
    "You are required to report the precision and recall for each category on test set and plot the training loss and validation loss for both SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b526d",
   "metadata": {},
   "source": [
    "##### Please run the following cell to evaluate your model with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a3da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_hat = sgd_lr.predict(x_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "precision = precision_score(y_true, y_hat, average=None)\n",
    "recall = recall_score(y_true, y_hat, average=None)\n",
    "\n",
    "print('SGD')\n",
    "print()\n",
    "print('  Precision:')\n",
    "print(f'    class {0}: {precision[0]:.4f}, class {1}: {precision[1]:.4f}, class {2}: {precision[2]:.4f}, class {3}: {precision[3]:.4f}')\n",
    "print()\n",
    "print('  Recall:')\n",
    "print(f'    class {0}: {recall[0]:.4f}, class {1}: {recall[1]:.4f}, class {2}: {recall[2]:.4f}, class {3}: {recall[3]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11a9db",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot training loss and validation loss for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d060606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(range(num_epoch), sgd_train_losses, sgd_valid_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.title('SGD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320f259",
   "metadata": {},
   "source": [
    "##### Please run the following cell to evaluate your model with Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = mini_gd_lr.predict(x_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "precision = precision_score(y_true, y_hat, average=None)\n",
    "recall = recall_score(y_true, y_hat, average=None)\n",
    "\n",
    "print('Mini-batch GD')\n",
    "print()\n",
    "print('  Precision:')\n",
    "print(f'    class {0}: {precision[0]:.4f}, class {1}: {precision[1]:.4f}, class {2}: {precision[2]:.4f}, class {3}: {precision[3]:.4f}')\n",
    "print()\n",
    "print('  Recall:')\n",
    "print(f'    class {0}: {recall[0]:.4f}, class {1}: {recall[1]:.4f}, class {2}: {recall[2]:.4f}, class {3}: {recall[3]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e123876",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot training loss and validation loss for Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_epoch), mini_gd_train_losses, mini_gd_valid_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.title('Mini-batch GD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57c4f4",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation (10 points)\n",
    "\n",
    "You are required to implement cross-validation, and use it to choose the best $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48591a",
   "metadata": {},
   "source": [
    "### 3.1 Reload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e47e48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = train_df['text'].values.astype(str)\n",
    "label_train = train_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "text_test = test_df['text'].values.astype(str)\n",
    "label_test = test_df['label'].values.astype(int) - 1 # -1 because labels start from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13eaab",
   "metadata": {},
   "source": [
    "### 3.2 Define the range of $\\lambda$. (Fill the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0499d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.001, 0.01, 0.1] ## Fill the values of lambda you want to evaluate in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f935bc0",
   "metadata": {},
   "source": [
    "### 3.3 Imlement cross-validation. (Fill the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3054dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start your code\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "splits = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for i in lambdas:\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for train_index, valida_index in splits.split(text_train):\n",
    "        # define training and validation sets\n",
    "        x_train , x_valida = text_train[train_index,:], text_train[valida_index, :]\n",
    "        y_train, y_valida = label_train[train_index], label_train[valida_index]\n",
    "        \n",
    "        # train model with minibatch \n",
    "        mini_gd_train_losses, mini_gd_valid_losses = mini_batch_gd(mini_gd_lr, text_train, label_train, batch_size, lr, i, num_epoch)\n",
    "        \n",
    "        train_loss.append(mini_gd_train_losses[-1])\n",
    "        \n",
    "        val_loss.append(mini_gd_valid_losses[-1])\n",
    "        \n",
    "        print(\"Mean_trainloss \",mean(train_loss) )\n",
    "        print(\"Valid_trainloss \",mean(val_loss) )\n",
    "\n",
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4adcbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "[ 0  1  2  3  4  6  7  8 10] [ 5  9 11]\n",
      "[ 1  2  3  5  6  8  9 10 11] [0 4 7]\n",
      "[ 0  1  3  4  5  6  7  8  9 11] [ 2 10]\n",
      "[ 0  2  3  4  5  6  7  9 10 11] [1 8]\n",
      "[ 0  1  2  4  5  7  8  9 10 11] [3 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "splits = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "x = np.arange(12)\n",
    "print(x)\n",
    "\n",
    "for train_index, valida_index in splits.split(x):\n",
    "    print(train_index,valida_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a402a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f0e386",
   "metadata": {},
   "source": [
    "### 3.4 Report the best $lambda$ value, and report the recall and precision for each category on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0950f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "my logistic regression class may not work, but show some ideas\n",
    "if the model work, probably, the best lambda value is 0.01.\n",
    "\n",
    "to use LG with SGD could be the best, cause selecting the random example may balance the recall and precision for each category.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec31af",
   "metadata": {},
   "source": [
    "# 4. Conclusion\n",
    "\n",
    "provide an analysis for the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37c28c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
