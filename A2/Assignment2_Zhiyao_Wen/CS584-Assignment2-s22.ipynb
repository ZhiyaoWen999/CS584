{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13133af",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: (Zhiyao Wen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61354aed",
   "metadata": {},
   "source": [
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement MLP.\n",
    "3. Implement Word2Vec\n",
    "\n",
    "*** Please read the code and comments very carefully and install these packages (NumPy, Pandas, sklearn, tqdm, spacy, and matplotlib) before you start ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae74ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#                                                             #\n",
    "#    Run this cell to make sure all packages are installed.   #\n",
    "#                                                             #\n",
    "###############################################################\n",
    "\n",
    "# !pip install numpy pandas scikit-learn tqdm matplotlib\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d986acc",
   "metadata": {},
   "source": [
    "## 1. MLP (30 points)\n",
    "In this section, you are required to implement the MLP in the following steps.\n",
    "1. Data Processing (Same as assignment 1, you could directly use your code in assignment 1.)\n",
    "2. MLP\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366131e1",
   "metadata": {},
   "source": [
    "### 1.1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444c63b",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37217a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# training data\n",
    "train_df = pd.read_csv('./data/train.csv', header=None)\n",
    "train_df.columns = ['label', 'title', 'text']\n",
    "\n",
    "# test data\n",
    "test_df = pd.read_csv('./data/test.csv', header=None)\n",
    "test_df.columns = ['label', 'title', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c181c",
   "metadata": {},
   "source": [
    "####  Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8eb2e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "class Preprocesser(object):\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "    \n",
    "    def apply(self, text):\n",
    "        \n",
    "        text = self._lowercase(text)\n",
    "        \n",
    "        if self.url:\n",
    "            text = self._remove_url(text)\n",
    "            \n",
    "        if self.punctuation:\n",
    "            text = self._remove_punctuation(text)\n",
    "            \n",
    "        if self.number:\n",
    "            text = self._remove_number(text)\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "        \n",
    "    def _remove_punctuation(self, text):\n",
    "        ''' Please fill this function to remove all the punctuations in the text\n",
    "        '''\n",
    "        ### Start your code\n",
    "        \n",
    "        text = re.sub(r'[^\\w\\s]', ' ',text) # remove punctuation with regular expression\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_url(self, text):\n",
    "        ''' Please fill this function to remove all the urls in the text\n",
    "        '''\n",
    "        ### Start your code\n",
    "        \n",
    "        text = re.sub(r'http\\S+', '', text) # remove url with regular expression\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_number(self, text):\n",
    "        ''' Please fill this function to remove all the numbers in the text\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        text = re.sub(\"(\\s\\d+)\",\"\",text) # remove number in text with regular expression\n",
    "        \n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _lowercase(self, text):\n",
    "        ''' Please fill this function to lower the text\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        text = text.lower() # change text char to lower case\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999e33b",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b02a15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(text):\n",
    "    ''' Please fill this function to tokenize text.\n",
    "            1. Tokenize the text.\n",
    "            2. Remove stop words.\n",
    "            3. Optional: lemmatize words accordingly.\n",
    "    '''\n",
    "    \n",
    "    ### Start your code\n",
    "    tokens = text.split()\n",
    "    \n",
    "    stop_words = stopwords.words() # to remove stop words\n",
    "    \n",
    "    # Remove any nonalphabetic, stopwords,or single letter words\n",
    "    \n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words and len(token) > 1]\n",
    "\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a79dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faa030fb",
   "metadata": {},
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ab0c0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set: 96000\n",
      "The size of validation set: 24000\n",
      "The size of testing set: 7600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train = train_df['text'].values.astype(str)\n",
    "label_train = train_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "text_test = test_df['text'].values.astype(str)\n",
    "label_test = test_df['label'].values.astype(int) - 1 # -1 because labels start from 1\n",
    "\n",
    "\n",
    "text_train, text_valid, label_train, label_valid = train_test_split(text_train, label_train, test_size=0.2)\n",
    "\n",
    "\n",
    "print('The size of training set:', text_train.shape[0])\n",
    "print('The size of validation set:', text_valid.shape[0])\n",
    "print('The size of testing set:', text_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6406dc6",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61963195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class TfIdfExtractor(object):\n",
    "    \n",
    "    def __init__(self, vocab_size=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.vocab = defaultdict(lambda: 0)\n",
    "        self.word2idx = {}\n",
    "        self.df = defaultdict(lambda: 0)\n",
    "        self.num_doc = 0\n",
    "        \n",
    "        self.processer = Preprocesser()\n",
    "        \n",
    "        \n",
    "    def fit(self, texts):\n",
    "        ''' In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary (self.vocab).\n",
    "                2. Construct the document frequency dictionary (self.df).\n",
    "                3. Sort the tokens(the keys in self.vocab) based on the frequence (the values of self.vocab).\n",
    "            Input:\n",
    "                texts: a list of text (training set)\n",
    "            Output:\n",
    "                None\n",
    "        '''\n",
    "\n",
    "        self.num_doc = len(texts)\n",
    "        \n",
    "        for text in tqdm(texts, desc='fitting text'):\n",
    "            clean_text = self.processer.apply(text)\n",
    "            tokens = tokenize(clean_text)\n",
    "            ### Start your code (step 1 & 2)\n",
    "            \n",
    "            for word in tokens:\n",
    "                # construct the vocabulary\n",
    "                self.vocab[word] += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            for key in self.vocab:\n",
    "                # count the number of document that words appear\n",
    "                if tokens.count(key) >0:\n",
    "                    self.df[key] += 1\n",
    "                    \n",
    "                \n",
    "            ### End\n",
    "        \n",
    "        ### Start your code (Step 3)\n",
    "        \n",
    "        # sort vocab\n",
    "        self.vocab = sorted(self.vocab.items(), key = lambda x:x[1],reverse= True)\n",
    "        \n",
    "        # convert to dic\n",
    "        self.vocab = {key:value for key, value in self.vocab}\n",
    "\n",
    "        self.word2idx = {key: idx for idx, key in enumerate(self.vocab.keys())}\n",
    "        ### End\n",
    "        \n",
    "        \n",
    "        if self.vocab_size is not None:\n",
    "            self.vocab = {key: self.vocab[key] for key in list(self.vocab.keys())[:self.vocab_size]}\n",
    "        \n",
    "        self.word2idx = {key: idx for idx, key in enumerate(self.vocab.keys())}\n",
    "\n",
    "\n",
    "    def transform(self, texts):\n",
    "        ''' In this function, you need to encode the input text into TF-IDF vector.\n",
    "            Input:\n",
    "                texts: a list of text.\n",
    "            Ouput:\n",
    "                a N-d matrix (Tf-Idf) \n",
    "        '''\n",
    "        #tfidf = np.zeros((len(texts), len(self.vocab)))\n",
    "        tf = np.zeros((len(texts), len(self.vocab)))\n",
    "        idf = np.zeros((1,len(self.vocab)))\n",
    "        \n",
    "        \n",
    "        for i, text in tqdm(enumerate(texts), desc='transforming', total=len(texts)):\n",
    "            clean_text = self.processer.apply(text)\n",
    "            tokens = tokenize(clean_text)\n",
    "            \n",
    "            ### Start your code\n",
    "            \n",
    "            for word, j in self.word2idx.items():\n",
    "                \n",
    "                tf[i][j] = tokens.count(word)/len(tokens) # from the function below\n",
    "                idf[0][j] = np.log(self.num_doc/(self.df[word]))\n",
    "                \n",
    "            ### End\n",
    "        tfidf = tf * idf\n",
    "        \n",
    "        return tfidf\n",
    "\n",
    "#     ### NEED TO BE REMOVED\n",
    "#     def _tf_idf(self, token, tokens):\n",
    "#         tf = tokens.count(token) / len(tokens)\n",
    "#         idf = np.log(self.num_doc / self.df[token])\n",
    "#         return tf * idf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f889a5c",
   "metadata": {},
   "source": [
    "#### Obtain the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a506079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c793631a37a4cdb86c4843a7f464c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fitting text:   0%|          | 0/96000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a8b433528d446ca75ce74af4ab9867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/96000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa2b3ff90384dbcb185f7f773db2bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662323a4dcfc43848f3a3c65df4b62c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transforming:   0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set: (96000, 4000)\n",
      "The size of validation set: (24000, 4000)\n",
      "The size of test set: (7600, 4000)\n"
     ]
    }
   ],
   "source": [
    "# You can change this number to see the difference of the performances. (larger vocab size needs more memory)\n",
    "vocab_size = 4000 \n",
    "num_class = 4\n",
    "\n",
    "extractor = TfIdfExtractor(vocab_size=vocab_size)\n",
    "extractor.fit(text_train)\n",
    "\n",
    "x_train = extractor.transform(text_train)\n",
    "x_valid = extractor.transform(text_valid)\n",
    "x_test = extractor.transform(text_test)\n",
    "\n",
    "\n",
    "# convert label to one-hot vector\n",
    "y_train = np.zeros((label_train.shape[0], num_class))\n",
    "y_train[np.arange(label_train.shape[0]), label_train] = 1\n",
    "\n",
    "y_valid = np.zeros((label_valid.shape[0], num_class))\n",
    "y_valid[np.arange(label_valid.shape[0]), label_valid] = 1\n",
    "\n",
    "y_test = np.zeros((label_test.shape[0], num_class))\n",
    "y_test[np.arange(label_test.shape[0]), label_test] = 1\n",
    "\n",
    "\n",
    "print('The size of training set:', x_train.shape)\n",
    "print('The size of validation set:', x_valid.shape)\n",
    "print('The size of test set:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053775b3",
   "metadata": {},
   "source": [
    "### 1.2 MLP (30 points)\n",
    "\n",
    "In this section, you are required to implement a 1-layer MLP from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa309f",
   "metadata": {},
   "source": [
    "#### 1.2.1 Implement MLP (fill the code: 20 points)\n",
    "\n",
    "> $z_1 = w_1x$\n",
    "\n",
    "> $h_1 = activation(z_1)$\n",
    "\n",
    "> $z_2 = w_2 h_1$\n",
    "\n",
    "> $\\hat{y} = softmax(z_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "782a59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37226315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, num_feature, hidden_size, num_class):\n",
    "        ''' Initialize the weight of MLP.\n",
    "            Inputs:\n",
    "                num_feature: scalar, the number of features (in this case, it is the vocab size).\n",
    "                hidden_size: scaler, the number of neurons in the hidden layer.\n",
    "                num_class: scalar, the number of classes.\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        self.num_feature = num_feature\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.w1 = np.random.randn(num_feature, hidden_size)\n",
    "        self.w2 = np.random.randn(hidden_size, num_class)\n",
    "        \n",
    "        self.b1 = np.random.randn(1,hidden_size)\n",
    "        self.b2 = np.random.randn(1,num_class)\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Implement the forward pass.\n",
    "            Input:\n",
    "                x: N-d matrix\n",
    "            Outputs\n",
    "                y_hat: the output of the model, N-K matrix.\n",
    "                h1: the output of the first hidden layer.\n",
    "                z1: the output of the first hidden before activation function.\n",
    "                \n",
    "                Note that the reason for return h1 and z1 is for calculating the gradient of self.w1 and self.w2.\n",
    "                Feel free to change it accordingly.\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        \n",
    "        \n",
    "        # input - hidden layer\n",
    "        z1 = x.dot(self.w1) + self.b1\n",
    "        \n",
    "        # after activation function\n",
    "        h1 = self.activation(z1)\n",
    "        \n",
    "        # hidden layer - output\n",
    "        z2 = h1.dot(self.w2) + self.b2\n",
    "        \n",
    "        #out y_hat\n",
    "        y_hat = self.softmax(z2)\n",
    "        \n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return y_hat, (h1, z1)\n",
    "    \n",
    "    \n",
    "    def backward(self, lr, x, y, y_hat, h1, z1):\n",
    "        ''' Implement back-propagation.\n",
    "            Inputs:\n",
    "                lr: learning rate.\n",
    "                x: the input, N-d matrix.\n",
    "                y_hat: the output, N-K matrix.\n",
    "                y: ground truth (N-K one-hot matrix).\n",
    "                h1: the output of the first hidden layer.\n",
    "                z1: the output of the first hidden before activation function.\n",
    "        '''\n",
    "\n",
    "        # Get the gradient of w1 and w2\n",
    "        grad_w1, grad_w2 = self.gradient(x, y, y_hat, h1, z1)\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.w1 -= lr * grad_w1\n",
    "        self.w2 -= lr * grad_w2\n",
    "        \n",
    "    \n",
    "    def objective(self, y, y_hat):\n",
    "        ''' Compute the loss\n",
    "            Inputs:\n",
    "                y: N-K matrix, ground truth.\n",
    "                y_hat: N-K matrix, prediction.\n",
    "            Output:\n",
    "                loss: scalar, the loss of the model.\n",
    "        '''\n",
    "        \n",
    "        loss = 0.\n",
    "        \n",
    "        ### Start your code\n",
    "        n = y.shape[0]\n",
    "        \n",
    "        loss = -1/n * np.sum(y * np.log(y_hat))\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def gradient(self, x, y, y_hat, h1, z1):\n",
    "        ''' Compute the gradient of self.w1 and self.w2\n",
    "            Inputs:\n",
    "                x: the input, N-d matrix.\n",
    "                y_hat: the output, N-K matrix.\n",
    "                y: ground truth (N-K one-hot matrix).\n",
    "                h1: the output of the first hidden layer.\n",
    "                z1: the output of the first hidden before activation function.\n",
    "            Outputs:\n",
    "                grad_w1: the gradient of self.w1.\n",
    "                grad_w2: the gradient of self.w2.\n",
    "        '''\n",
    "        n = x.shape[0]\n",
    "        \n",
    "        grad_w1 = 0. # 0 is a placeholder, the actual value should be a matrix\n",
    "        grad_w2 = 0. # 0 is a placeholder, the actual value should be a matrix\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        # gradient for w2\n",
    "        grad_w2 = -1/n * h1.T.dot(y - y_hat)\n",
    "        \n",
    "\n",
    "        # gradient for w1\n",
    "        grad_w1 = x.T.dot( np.where(z1 > 0,1,0) * 1/n * (y - y_hat).dot(self.w2.T))\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return grad_w1, grad_w2\n",
    "    \n",
    "    \n",
    "    def activation(self, x):\n",
    "        ''' Implement an activation function, ReLU or Sigmoid.\n",
    "            Please note that for different activation functions, \n",
    "            you have to implement different gradient in function \"backward()\"\n",
    "            \n",
    "            Input:\n",
    "                x: N-d matrix\n",
    "            Output:\n",
    "                x: sigmoid(x) or ReLU(x)\n",
    "        '''\n",
    "        \n",
    "        ### Start your code\n",
    "        \n",
    "        # Relu function\n",
    "        x = x * (x > 0) \n",
    "\n",
    "        \n",
    "        ### End\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def softmax(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x - np.max(x)\n",
    "            return np.exp(x) / np.sum(np.exp(x))\n",
    "        else:\n",
    "            x = x - np.max(x, axis=1).reshape(-1, 1)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ce97f",
   "metadata": {},
   "source": [
    "#### Gradient Checking\n",
    "Please run the following cell to do the gradient checking. This block will check your implementation of calculating the gradient of w1 and w2. Make sure you pass this test, otherwise you can not train your model correctly.\n",
    "\n",
    "**Note that** if you don't pass this test case, please check the functions **\"MLP.gradient()\"** and **\"MLP.objective()\"** very carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bfa2c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#                                             #\n",
    "#   Checking functions below. DO NOT MODIFY   #\n",
    "#                                             #\n",
    "###############################################\n",
    "def gradient_checking(model, X, y, epsilon):    \n",
    "    y_hat, (h1, z1) = model.forward(X)\n",
    "    grad_w1, grad_w2 = model.gradient(X, y, y_hat, h1, z1)\n",
    "    \n",
    "    it = np.nditer(model.w2, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        model.w2[ix] += epsilon # increment by eps\n",
    "        y_hat, _ = model.forward(X)\n",
    "        l1 = model.objective(y, y_hat)\n",
    "        \n",
    "        model.w2[ix] -= 2 * epsilon # restore to previous value (very important!)\n",
    "        y_hat, _ = model.forward(X)\n",
    "        l2 = model.objective(y, y_hat)\n",
    "        \n",
    "        model.w2[ix] += epsilon\n",
    "        numgrad = (l1 - l2) / 2 / epsilon\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad_w2[ix])\n",
    "        if reldiff > 1e-4:\n",
    "            print(\"Gradient check failed for w2.\")\n",
    "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad_w2[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "        \n",
    "    it = np.nditer(model.w1, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        model.w1[ix] += epsilon # increment by eps\n",
    "        y_hat, _ = model.forward(X)\n",
    "        l1 = model.objective(y, y_hat)\n",
    "        \n",
    "        model.w1[ix] -= 2 * epsilon # restore to previous value (very important!)\n",
    "        y_hat, _ = model.forward(X)\n",
    "        l2 = model.objective(y, y_hat)\n",
    "        \n",
    "        model.w1[ix] += epsilon\n",
    "        numgrad = (l1 - l2) / 2 / epsilon\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad_w1[ix])\n",
    "        if reldiff > 1e-4:\n",
    "            print(\"Gradient check failed for w1.\")\n",
    "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad_w1[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "        \n",
    "    print(\"Gradient check passed!\")\n",
    "    \n",
    "epsilon = 1E-4\n",
    "np.random.seed(10)\n",
    "dummy_X = np.random.rand(10, 5)\n",
    "dummy_y = np.eye(3)[np.random.choice(3, 10)]\n",
    "dummy_model = MLP(5, 3, 3)\n",
    "gradient_checking(dummy_model, dummy_X, dummy_y, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79382a67",
   "metadata": {},
   "source": [
    "#### 1.2.2 Optimization (Fill the code: 10 points)\n",
    "\n",
    "In this section, you are requried to call the MLP model to implement the Mini-batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0290bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(model, X, y, lr, batch_size=None, num_epoch=100):\n",
    "    ''' Implement Mini-batch GD\n",
    "        Inputs:\n",
    "            X: N-d matrix\n",
    "            y: N vector\n",
    "            lr: learning rate\n",
    "            batch_size: optional, depends on if you use Mini-batch GD\n",
    "            num_epoch: the number of epochs\n",
    "        Output:\n",
    "            A list of training losses against epoch\n",
    "            A list of validation losses against epoch\n",
    "    '''\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    n, _ = X.shape\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        train_loss = 0.\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        rand_indices = np.random.permutation(n)\n",
    "        x_rand = X[rand_indices]\n",
    "        y_rand = y[rand_indices]\n",
    "        \n",
    "        for b in tqdm(range(0, n, batch_size), f'Epoch {e+1}/{num_epoch}'):\n",
    "            x_batch = x_rand[b: b+batch_size] \n",
    "            y_batch = y_rand[b: b+batch_size]\n",
    "            \n",
    "            ### Start your code\n",
    "            \n",
    "            ### Step 1, call forward function to get outputs\n",
    "            \n",
    "            y_hat, (h1, z1) = model.forward(x_batch)\n",
    "            \n",
    "            ### Step 2, call objective function to get loss\n",
    "            \n",
    "            loss = model.objective(y_batch,y_hat)\n",
    "            \n",
    "            ### Step 3, call backward to update the weights\n",
    "            \n",
    "            model.backward(lr, x_batch, y_batch, y_hat, h1, z1)\n",
    "            \n",
    "            # End\n",
    "            \n",
    "            losses.append(loss)\n",
    "        \n",
    "        train_loss = np.mean(losses)\n",
    "        \n",
    "        \n",
    "        y_hat, _ = model.forward(x_valid)\n",
    "        valid_loss = model.objective(y_valid, y_hat)\n",
    "        \n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(f'At epoch {e+1}, training loss: {train_loss:.4f}, validation loss: {valid_loss:.4f}.')\n",
    "            \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a90ff1",
   "metadata": {},
   "source": [
    "#### Run the following cell to train the model. (Feel free to tune the hpyer-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3359ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd94154788c4044a8b7b997c3403263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1, training loss: 4.5386, validation loss: 4.0285.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27ed3a19499436e89e121a9f898f42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 2, training loss: 3.7773, validation loss: 3.4935.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0679d8b5d127430c8ef507f59d0c523d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 3, training loss: 3.2748, validation loss: 3.0367.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725c172bfd5a4441a631bb7fed25a02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 4, training loss: 2.8574, validation loss: 2.6659.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b29618e31445c292c4f48c5fd27ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 5, training loss: 2.5269, validation loss: 2.3782.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70734d64fb4044d6a32c7472e19b60af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 6, training loss: 2.2740, validation loss: 2.1595.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e739a1f2b349aeade31d944785b8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 7, training loss: 2.0822, validation loss: 1.9950.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adea8a81b2be463cb8ff0f5c4d89743a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 8, training loss: 1.9363, validation loss: 1.8679.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f85375d349d427bb4709e6ce5f736c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 9, training loss: 1.8229, validation loss: 1.7688.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cd0fdb3bd5475e857829edf7a4e516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 10, training loss: 1.7333, validation loss: 1.6901.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d6af249e1c4626b9beb9f016b1cbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 11, training loss: 1.6615, validation loss: 1.6273.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dd8a24d3d94085bbcb17bd8c117092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 12, training loss: 1.6035, validation loss: 1.5762.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0856f0cab0624d2e89d307930a7dc28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 13, training loss: 1.5568, validation loss: 1.5349.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad0284df5c54698ab2f70dda1d74306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 14, training loss: 1.5187, validation loss: 1.5019.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491f98a1d5fb4433ad6b7f4b2a401c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 15, training loss: 1.4877, validation loss: 1.4746.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf84abbe5e8427bb3d03b0278c50739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 16, training loss: 1.4625, validation loss: 1.4524.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b4727571f2426eb3a87c9e4c7f38ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 17, training loss: 1.4420, validation loss: 1.4346.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ea9be137e3447db38e9ee7b297b0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 18, training loss: 1.4252, validation loss: 1.4195.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102d0ba1954040709dbe4612b82fc636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 19, training loss: 1.4114, validation loss: 1.4076.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81380b631a4b462da12c2550797f016c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 20, training loss: 1.3999, validation loss: 1.3978.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4912790bf6411e9ff24ff7174b1c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 21, training loss: 1.3905, validation loss: 1.3890.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5581ad5716d54db1870ea99e8e46e270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 22, training loss: 1.3825, validation loss: 1.3821.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9211b3fe40b4f11af89b2520642499a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 23, training loss: 1.3759, validation loss: 1.3763.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3732d3b72b7e4e48919e613dd58c4988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 24, training loss: 1.3701, validation loss: 1.3711.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6782e4cdf2204cbcb3fef0e509d7c603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 25, training loss: 1.3654, validation loss: 1.3668.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5951b33103d41538570bf94583e94ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 26, training loss: 1.3613, validation loss: 1.3631.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097558dc1d334658a46dda2ae17b7a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 27, training loss: 1.3578, validation loss: 1.3602.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807c30ed5c414ed681aadf28db18b6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 28, training loss: 1.3549, validation loss: 1.3575.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34b87982fe0476dba7ca917c64bc882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 29, training loss: 1.3523, validation loss: 1.3550.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54159fdfc6bb49fc9ab848093f897c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 30, training loss: 1.3501, validation loss: 1.3530.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282c3eb4add244bdb4e773a79994b234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 31/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 31, training loss: 1.3482, validation loss: 1.3512.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7fdbbe4c9b465cb95c5ea78435eb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 32/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 32, training loss: 1.3465, validation loss: 1.3499.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3827eb713c4d91b3660b6765121509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 33/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 33, training loss: 1.3451, validation loss: 1.3487.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9dc179215a4416abb3b7a8ff0288102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 34/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 34, training loss: 1.3439, validation loss: 1.3473.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025c58f0359345a88608eccf455473e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 35/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 35, training loss: 1.3428, validation loss: 1.3463.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f748dd45669f4101adc9eefcbf45e233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 36/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 36, training loss: 1.3419, validation loss: 1.3459.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa4bb05cb2b45f0a4d88ba1c95f723e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 37/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 37, training loss: 1.3412, validation loss: 1.3447.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d523a52ad746daa3cc999a396027fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 38/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 38, training loss: 1.3404, validation loss: 1.3447.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88acf4236c6e44e48ac296b9f84af479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 39/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 39, training loss: 1.3399, validation loss: 1.3438.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1a2b5f0994412b86460eba39868270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 40/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 40, training loss: 1.3395, validation loss: 1.3431.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2745ca966eec4b04b61c42385fa00057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 41/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 41, training loss: 1.3391, validation loss: 1.3426.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addf71c476eb472599d4042d121f61a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 42/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 42, training loss: 1.3388, validation loss: 1.3422.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62070816f97433d8ad542c6f428bc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 43/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 43, training loss: 1.3385, validation loss: 1.3419.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836ad8f437be48819d4a73e195620b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 44/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 44, training loss: 1.3382, validation loss: 1.3417.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29f2e15d9f94c3aa6d1f0a14fe9b10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 45/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 45, training loss: 1.3381, validation loss: 1.3420.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28829e46a8b44e89341f64c8f1c1b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 46/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 46, training loss: 1.3380, validation loss: 1.3415.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87ff65ec72b4d448f5ca890f6066715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 47/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 47, training loss: 1.3379, validation loss: 1.3413.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8686210c074f438f6799e4db5562a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 48/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 48, training loss: 1.3379, validation loss: 1.3412.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58145af671cc48e39d499a4ff5420e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 49/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 49, training loss: 1.3379, validation loss: 1.3410.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7207866a934224a11a39503cd4bdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 50/50:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 50, training loss: 1.3379, validation loss: 1.3412.\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 50\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_size = 50\n",
    "\n",
    "model = MLP(vocab_size, hidden_size, num_class)\n",
    "train_losses, valid_losses = optimization(model, x_train, y_train, lr, batch_size=batch_size, num_epoch=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6d8ed",
   "metadata": {},
   "source": [
    "#### 1.2.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080353a",
   "metadata": {},
   "source": [
    "#### Run the following cell to evaluate the performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33516692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7600,)\n",
      "MLP\n",
      "\n",
      "  Precision:\n",
      "    class 0: 0.3841, class 1: 0.3698, class 2: 0.3433, class 3: 0.3435\n",
      "\n",
      "  Recall:\n",
      "    class 0: 0.4005, class 1: 0.3505, class 2: 0.3574, class 3: 0.3326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "y_hat, _ = model.forward(x_test)\n",
    "y_hat = np.argmax(y_hat, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(y_true.shape)\n",
    "precision = precision_score(y_true, y_hat, average=None)\n",
    "recall = recall_score(y_true, y_hat, average=None)\n",
    "\n",
    "print('MLP')\n",
    "print()\n",
    "print('  Precision:')\n",
    "print(f'    class {0}: {precision[0]:.4f}, class {1}: {precision[1]:.4f}, class {2}: {precision[2]:.4f}, class {3}: {precision[3]:.4f}')\n",
    "print()\n",
    "print('  Recall:')\n",
    "print(f'    class {0}: {recall[0]:.4f}, class {1}: {recall[1]:.4f}, class {2}: {recall[2]:.4f}, class {3}: {recall[3]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee2d0a",
   "metadata": {},
   "source": [
    "#### Run the following cell to plot the training loss and validation loss against epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d22f0f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(range(num_epoch), train_losses, valid_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.title('SGD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ffe7e",
   "metadata": {},
   "source": [
    "## 2. Word Vector (70 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065035d",
   "metadata": {},
   "source": [
    "In this section, you are required to implement a Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5abb7",
   "metadata": {},
   "source": [
    "### 2.1 Written Questions (25 points, each question is 5 points. )\n",
    "\n",
    "\n",
    "To better understand the insight of Word2Vec, Please answer the following questions. (You can either answer those questions in this notebook, or submit a pdf with your answers.)\n",
    "\n",
    "1. Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value (i.e., in some expressions where only $\\sigma(x)$, but not $x$, is present). Assume that the input $x$ is a scalar for this question. Recall, the sigmoid function is:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "2. Assume you are given a predicted word vector $\\mathbf{v}_c$ corresponding to the center word $c$ for skipgram, and the word prediction is made with the **softmax** function\n",
    "\n",
    "<center>$\\hat{y}_o = p(o|c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{w=1}^W \\exp(\\mathbf{u}_w^\\top\\mathbf{v}_c)}$</center>\n",
    "\n",
    "> where $o$ is the expected word, $w$ denotes the $w$-th word and $\\mathbf{u}_w$ (w = 1, ..., W) are the output word vectors for all words in the vocabulary.\n",
    "The cross entropy function is defined as:\n",
    "\n",
    "<center>$J_\\text{CE}(o,\\mathbf{v}_c, U) =CE(\\mathbf{y}, \\hat{\\mathbf{y}})= -\\sum_i y_i \\log(\\hat{y}_i)$</center>\n",
    "\n",
    "> where the gold vector $\\mathbf{y}$ is a one-hot vector, the softmax prediction vector $\\hat{\\mathbf{y}}$ is a\n",
    "probability distribution over the output space, and $U= [u_1, u_2, ..., u_W]$ is the matrix of all the output vectors. \n",
    "Assume cross entropy cost is applied to this prediction, derive the gradients with respect to $\\mathbf{v}_c$.\n",
    "\n",
    "3. Derive gradients for the \"output\" word vector $\\mathbf{u}_w$ (including $\\mathbf{u}_o$) in the previous part.\n",
    "\n",
    "\n",
    "4. Repeat (2) and (3) assuming we are using the negative sampling loss for the predicted vector $\\mathbf{v}_c$. Assume that K negative samples (words) are drawn and they are 1,...,K respectively. For simplicity of notation, assume ($o\\notin \\{1,...,K\\}$). Again for a given word $o$, use $\\mathbf{u}_o$ to denote its output vector. The negative sampling loss function in this case is:\n",
    "$$J_\\text{neg-sample}(o,\\mathbf{v}_c, U) =-\\log(\\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)) -\\sum_{k=1}^K \\log(\\sigma(-\\mathbf{u}_k^\\top \\mathbf{v}_c))$$\n",
    "\n",
    "5. Derive gradients for all of the word vectors for skip-gram given the previous parts and given a set of context words $[\\text{word}_{c-m}, . . . , \\text{word}_c, . . . , \\text{word}_{c+m}]$ where $m$ is the\n",
    "context size. Denote the \"input\" and \"output\" word vectors for word $k$ as $\\mathbf{v}_k$ and $\\mathbf{u}_k$ respectively.\n",
    "\n",
    "> _Hint: feel free to use $F(o, \\mathbf{v}_c)$ (where $o$ is the expected word) as a placeholder for the $J_\\text{CE}(o,\\mathbf{v}_c ...)$ or $J_\\text{neg-sample}(o,\\mathbf{v}_c...)$ cost functions in this part -- youll see that this is a useful abstraction for the coding part. That is, your solution may contain terms of the form $\\frac{\\partial F(o, \\mathbf{v}_c)}{\\partial ...}$ Recall that for skip-gram, the cost for a context centered around c is:\n",
    "$$\\sum_{-m \\leq j\\leq m, j\\neq0} F(w_{c+j}, \\mathbf{v}_c)$$_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d3cef",
   "metadata": {},
   "source": [
    "### 2.2 Coding (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99661b35",
   "metadata": {},
   "source": [
    "#### 2.2.1 Sigmoid Function (Fill the code: 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed85bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    ''' Compute the sigmoid function.\n",
    "        Inputs:\n",
    "            x: A scalar or numpy array\n",
    "        Outputs:\n",
    "            s: sigmoid(x)\n",
    "    '''\n",
    "    s = 0.\n",
    "    \n",
    "    ### Start your code\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    ''' Compute the softmax function for each row of the input x. \n",
    "        It is crucial that this function is optimized for speed \n",
    "        because it will be used frequently in later code. \n",
    "\n",
    "        Inputs:\n",
    "        x: A D dimensional vector or N x D dimensional numpy matrix.\n",
    "        Outputs:\n",
    "        x: You are allowed to modify x in-place\n",
    "    '''\n",
    "    if x.ndim > 1:\n",
    "        x = np.exp(x) / np.sum(np.exp(x), axis=1)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        x = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bdc04f",
   "metadata": {},
   "source": [
    "#### 2.2.2 Word2Vec models with Stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f12b43",
   "metadata": {},
   "source": [
    "#### Naive Softmax loss & gradient function for word2vec models (fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b84ebcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset):\n",
    "    ''' Implement tha naive softmax loss and gradients between a center word's embedding \n",
    "        and an outside word's embedding. This will be the building block for our word2vec\n",
    "        models.\n",
    "        \n",
    "        Inputs:\n",
    "            centerWordVec: numpy ndarray, center word's embedding (v_c in question 2.1.1).\n",
    "            outsideWordIdx: integer, the index of the outside word (o of u_o in question 2.1.1).\n",
    "            outsideVectors: outside vectors (rows of matrix) for all words in vocab (U in question 2.1.1).\n",
    "            dataset: for negative sampling, ignore this argument in this function.\n",
    "        \n",
    "        Outputs:\n",
    "            loss: naive softmax loss\n",
    "            gradCenterVec: the gradient with respect to the center word vector (dJ/dv_c in question 2.1.1).\n",
    "            gradOutsideVecs: the gradient with respect to all the outside word vectors (dJ / dU).\n",
    "    '''\n",
    "    \n",
    "    ### Start your code (Please use the provided softmax function)\n",
    "    \n",
    "    y_hat = softmax(np.dot(outsideVectors, centerWordVec)) # y_hat vector\n",
    "    \n",
    "    #print(y_hat.shape)\n",
    "    loss = -np.log(y_hat[outsideWordIdx])  # compute the loss at index of the outside word\n",
    "    \n",
    "    diff = y_hat.copy() \n",
    "    \n",
    "    diff[outsideWordIdx] -= 1 # compute the difference of y and y_hat\n",
    "    \n",
    "    gradCenterVec = outsideVectors.T.dot(diff) # compute the difference and outside vectors\n",
    "    \n",
    "    gradOutsideVecs = np.outer(diff, centerWordVec) #compute gradient with all the outside word vectors, as N x D matrix\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cda58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d9087e",
   "metadata": {},
   "source": [
    "#### Negative sampling loss function for word2vec models (Fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20d42e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n",
    "\n",
    "def negSamplingLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset, K=10):\n",
    "    ''' Implement the negative sampling loss and gradients for a centerWordVec\n",
    "        and a outsideWordIdx word vector as a building block for word2vec\n",
    "        models. K is the number of negative samples to take.\n",
    "\n",
    "        Note: The same word may be negatively sampled multiple times. For\n",
    "        example if an outside word is sampled twice, you shall have to\n",
    "        double count the gradient with respect to this word. Thrice if\n",
    "        it was sampled three times, and so forth.\n",
    "\n",
    "        Inputs/Outpus Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    '''\n",
    "    \n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "    \n",
    "    ### Start your code (Please use your implementation of sigmoid)\n",
    "    \n",
    "    neg_vector = outsideVectors[negSampleWordIndices] # get the negative sample vector\n",
    "    \n",
    "    out_realword_pro = sigmoid(outsideVectors[outsideWordIdx].dot(centerWordVec)) # Probability of outside real word\n",
    "    \n",
    "    neg_word_pro = sigmoid(- outsideVectors[negSampleWordIndices].dot(centerWordVec))  # Probability of random negative word \n",
    "    \n",
    "    loss = -np.log(out_realword_pro) - np.sum(np.log(neg_word_pro)) # Calculate negative sampling loss\n",
    "    \n",
    "    #compute the gradient of center word vector\n",
    "    gradCenterVec = (out_realword_pro - 1) * outsideVectors[outsideWordIdx] + np.sum(((1 - neg_word_pro)[:, np.newaxis] * neg_vector),axis = 0)\n",
    "    \n",
    "    #initialize gradient\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "    \n",
    "    #compute the gradient of outside word index\n",
    "    gradOutsideVecs[outsideWordIdx] = (out_realword_pro - 1) * centerWordVec \n",
    "    \n",
    "    for i in range(len(negSampleWordIndices)):\n",
    "        #update the gradient with center word vectors\n",
    "        gradOutsideVecs[negSampleWordIndices[i]] += (1 - neg_word_pro[i]) * centerWordVec\n",
    "    ### End\n",
    "    \n",
    "    return loss, gradCenterVec, gradOutsideVecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead2711",
   "metadata": {},
   "source": [
    "#### Skip-gram model in word2vec (Fill the code, 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20929197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    ''' Implement the skip-gram model in this function.\n",
    "    \n",
    "        Inputs:\n",
    "            currentCenterWord: a string of the current center word\n",
    "            windowSize: integer, context window size\n",
    "            outsideWords: list of no more than 2*windowSize strings, the outside words\n",
    "            word2Ind: a dictionary that maps words to their indices in\n",
    "                      the word vector list\n",
    "            centerWordVectors: center word vectors (as rows) for all words in vocab\n",
    "                                (V in pdf handout)\n",
    "            outsideVectors: outside word vectors (as rows) for all words in vocab\n",
    "                            (U in pdf handout)\n",
    "            word2vecLossAndGradient: the loss and gradient function for\n",
    "                                       a prediction vector given the outsideWordIdx\n",
    "                                       word vectors, could be one of the two\n",
    "                                       loss functions you implemented above.\n",
    "\n",
    "        Outputs:\n",
    "            loss: the loss function value for the skip-gram model\n",
    "                    (J in the pdf handout)\n",
    "            gradCenterVecs: the gradient with respect to the center word vectors\n",
    "                    (dJ / dV in the pdf handout)\n",
    "            gradOutsideVectors: the gradient with respect to the outside word vectors\n",
    "                                (dJ / dU in the pdf handout)\n",
    "    '''\n",
    "    \n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    # Start your code\n",
    "    \n",
    "    center_index = word2Ind[currentCenterWord]   #  center word index\n",
    "    center_vecter = centerWordVectors[center_index] # center word vector\n",
    "    \n",
    "    outside_indices = [word2Ind[i] for i in outsideWords] # get outside word indices\n",
    "    \n",
    "    #Calutate the loss and graidents\n",
    "    for i in outside_indices:\n",
    "        \n",
    "        los, center_grad, outside_grad = word2vecLossAndGradient(center_vecter, i, outsideVectors, dataset)\n",
    "        \n",
    "        loss += los\n",
    "        gradCenterVecs[center_index] += center_grad\n",
    "        gradOutsideVectors += outside_grad\n",
    "    \n",
    "    \n",
    "    # End\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n",
    "\n",
    "\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N, 1)) + 1e-30\n",
    "    return x\n",
    "\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2), :]\n",
    "    outsideVectors = wordVectors[int(N/2):, :]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad467099",
   "metadata": {},
   "source": [
    "#### Run the following cell to test your implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41c61567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "Skip-Gram with naiveSoftmaxLossAndGradient\n",
      "Your Result:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "    \n",
      "Skip-Gram with negSamplingLossAndGradient\n",
      "Your Result:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "############################################\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from utils.gradcheck import gradcheck_naive\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0, 4)], \\\n",
    "            [tokens[random.randint(0, 4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "    dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(\"Skip-Gram with naiveSoftmaxLossAndGradient\")\n",
    "\n",
    "    print(\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                  dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n",
    "    )\n",
    "    )\n",
    "\n",
    "    print(\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 11.16610900153398\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-1.26947339 -1.36873189  2.45158957]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    "Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.41045956  0.18834851  1.43272264]\n",
    " [ 0.38202831 -0.17530219 -1.33348241]\n",
    " [ 0.07009355 -0.03216399 -0.24466386]\n",
    " [ 0.09472154 -0.04346509 -0.33062865]\n",
    " [-0.13638384  0.06258276  0.47605228]]\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Skip-Gram with negSamplingLossAndGradient\")\n",
    "    print(\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5, :],\n",
    "                  dummy_vectors[5:, :], dataset, negSamplingLossAndGradient)\n",
    "    )\n",
    "    )\n",
    "    print(\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 16.15119285363322\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-4.54650789 -1.85942252  0.76397441]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    " Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.69148188  0.31730185  2.41364029]\n",
    " [-0.22716495  0.10423969  0.79292674]\n",
    " [-0.45528438  0.20891737  1.58918512]\n",
    " [-0.31602611  0.14501561  1.10309954]\n",
    " [-0.80620296  0.36994417  2.81407799]]\n",
    "    \"\"\")\n",
    "    \n",
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee2525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a41a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3016073",
   "metadata": {},
   "source": [
    "#### 2.2.3 K-nearest neighbors. (Fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6daa22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similartiy(v1, v2):\n",
    "    ''' return the cosine similarity of two vectors\n",
    "        \n",
    "        Inputs:\n",
    "            v1: a numpy ndarray\n",
    "            v2: a numpy ndarray\n",
    "        Outputs:\n",
    "            s: the cosine similarity of v1 and v2\n",
    "    '''\n",
    "    \n",
    "    ### Start your code\n",
    "    \n",
    "    dot_product = v2.dot(v1)\n",
    "        \n",
    "    denominator  = (np.sqrt(np.sum(v1**2))) * (np.sqrt(np.sum(v2**2, axis=1)))\n",
    "        \n",
    "    s = dot_product/denominator\n",
    "    \n",
    "    ### End\n",
    "    \n",
    "    return s\n",
    "\n",
    "def knn(vec, mat, k):\n",
    "    ''' Implement the KNN algorithm, which will be used for analysis.\n",
    "        \n",
    "        Inputs:\n",
    "            vec: numpy ndarray, the target vector\n",
    "            mat: numpy ndarray, a matrix contains all the vectors (each row is a vector)\n",
    "            k: the number of the nearest neighbors you want to find.\n",
    "            \n",
    "        Outputs:\n",
    "            indices: the k indices of the matrix's rows that are closest to the vec\n",
    "    '''\n",
    "                          \n",
    "    ### Start your code\n",
    "                          \n",
    "    cos_sim = cosine_similartiy(vec, mat)\n",
    "    indices = (-cos_sim).argsort()[:k]  # sort the cosine similarity descendingly, as larger cosine similarity means closer distance.\n",
    "    ### End\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615fef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0900e217",
   "metadata": {},
   "source": [
    "#### 2.2.4 Evalution the model with visualization and knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cac7f6",
   "metadata": {},
   "source": [
    "#### Run the following cell to train the word2vec model\n",
    "\n",
    "_Note: The training process may take a long time depending on the efficiency of your implementation. Plan accordingly!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eefcf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "from utils.treebank import StanfordSentiment\n",
    "\n",
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        params_file = \"saved_params_%d.npy\" % st\n",
    "        state_file = \"saved_state_%d.pickle\" % st\n",
    "        params = np.load(params_file)\n",
    "        with open(state_file, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    params_file = \"saved_params_%d.npy\" % iter\n",
    "    np.save(params_file, params)\n",
    "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=1000):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a loss and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        def postprocessing(x): return x\n",
    "\n",
    "    exploss = None\n",
    "\n",
    "    last_time = time.time()\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        loss = None\n",
    "        loss, g = f(x)\n",
    "        x -= step * g\n",
    "\n",
    "        x = postprocessing(x)\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not exploss:\n",
    "                exploss = loss\n",
    "            else:\n",
    "                exploss = .95 * exploss + .05 * loss\n",
    "            print(\"iter %d: %f, duration %d\" % (iter, exploss, int(time.time() - last_time)))\n",
    "            last_time = time.time()\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b5d4bb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6b5c141c1945c3aa60692fec624b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100: 24.246306, duration 22\n",
      "iter 200: 24.139565, duration 16\n",
      "iter 300: 24.061029, duration 16\n",
      "iter 400: 23.994044, duration 16\n",
      "iter 500: 23.968544, duration 16\n",
      "iter 600: 24.066254, duration 16\n",
      "iter 700: 24.082872, duration 17\n",
      "iter 800: 23.923287, duration 16\n",
      "iter 900: 23.771688, duration 16\n",
      "iter 1000: 23.917420, duration 16\n",
      "iter 1100: 23.865197, duration 17\n",
      "iter 1200: 23.785051, duration 16\n",
      "iter 1300: 24.021437, duration 17\n",
      "iter 1400: 24.146951, duration 16\n",
      "iter 1500: 24.075635, duration 17\n",
      "iter 1600: 24.061075, duration 17\n",
      "iter 1700: 24.146272, duration 17\n",
      "iter 1800: 24.143120, duration 17\n",
      "iter 1900: 24.078559, duration 16\n",
      "iter 2000: 24.017543, duration 17\n",
      "iter 2100: 24.050359, duration 18\n",
      "iter 2200: 23.898852, duration 18\n",
      "iter 2300: 23.684494, duration 17\n",
      "iter 2400: 23.672454, duration 17\n",
      "iter 2500: 23.598513, duration 16\n",
      "iter 2600: 23.740478, duration 16\n",
      "iter 2700: 23.993217, duration 17\n",
      "iter 2800: 23.860472, duration 17\n",
      "iter 2900: 23.920916, duration 17\n",
      "iter 3000: 24.126551, duration 17\n",
      "iter 3100: 24.160508, duration 17\n",
      "iter 3200: 24.052774, duration 17\n",
      "iter 3300: 24.043455, duration 17\n",
      "iter 3400: 24.191900, duration 17\n",
      "iter 3500: 24.235694, duration 18\n",
      "iter 3600: 24.387058, duration 17\n",
      "iter 3700: 24.374042, duration 17\n",
      "iter 3800: 24.384344, duration 17\n",
      "iter 3900: 24.369947, duration 17\n",
      "iter 4000: 24.187430, duration 17\n",
      "iter 4100: 24.000945, duration 17\n",
      "iter 4200: 23.897564, duration 17\n",
      "iter 4300: 23.691118, duration 17\n",
      "iter 4400: 23.750776, duration 17\n",
      "iter 4500: 23.512882, duration 17\n",
      "iter 4600: 23.358998, duration 17\n",
      "iter 4700: 23.272847, duration 17\n",
      "iter 4800: 23.186009, duration 17\n",
      "iter 4900: 23.000433, duration 17\n",
      "iter 5000: 22.933736, duration 17\n",
      "iter 5100: 22.852776, duration 17\n",
      "iter 5200: 22.672733, duration 17\n",
      "iter 5300: 22.555384, duration 17\n",
      "iter 5400: 22.387985, duration 17\n",
      "iter 5500: 22.228242, duration 17\n",
      "iter 5600: 22.184572, duration 17\n",
      "iter 5700: 21.970381, duration 17\n",
      "iter 5800: 21.745639, duration 17\n",
      "iter 5900: 21.770541, duration 17\n",
      "iter 6000: 21.547797, duration 17\n",
      "iter 6100: 21.414677, duration 17\n",
      "iter 6200: 21.193390, duration 17\n",
      "iter 6300: 21.086617, duration 17\n",
      "iter 6400: 20.899953, duration 17\n",
      "iter 6500: 20.755448, duration 17\n",
      "iter 6600: 20.603655, duration 17\n",
      "iter 6700: 20.436880, duration 18\n",
      "iter 6800: 20.429233, duration 18\n",
      "iter 6900: 20.424649, duration 18\n",
      "iter 7000: 20.297914, duration 17\n",
      "iter 7100: 20.155251, duration 18\n",
      "iter 7200: 20.048829, duration 19\n",
      "iter 7300: 19.851290, duration 18\n",
      "iter 7400: 19.774494, duration 18\n",
      "iter 7500: 19.574787, duration 18\n",
      "iter 7600: 19.484235, duration 17\n",
      "iter 7700: 19.335817, duration 17\n",
      "iter 7800: 19.131701, duration 17\n",
      "iter 7900: 19.102131, duration 17\n",
      "iter 8000: 18.864694, duration 17\n",
      "iter 8100: 18.560162, duration 18\n",
      "iter 8200: 18.468472, duration 18\n",
      "iter 8300: 18.290826, duration 17\n",
      "iter 8400: 18.167535, duration 17\n",
      "iter 8500: 18.006950, duration 17\n",
      "iter 8600: 17.923392, duration 17\n",
      "iter 8700: 17.901067, duration 17\n",
      "iter 8800: 17.860354, duration 16\n",
      "iter 8900: 17.804728, duration 17\n",
      "iter 9000: 17.658688, duration 16\n",
      "iter 9100: 17.590154, duration 17\n",
      "iter 9200: 17.336325, duration 16\n",
      "iter 9300: 17.088656, duration 16\n",
      "iter 9400: 17.010306, duration 17\n",
      "iter 9500: 16.869896, duration 16\n",
      "iter 9600: 16.880132, duration 17\n",
      "iter 9700: 16.715495, duration 18\n",
      "iter 9800: 16.628288, duration 17\n",
      "iter 9900: 16.438263, duration 16\n",
      "iter 10000: 16.382256, duration 16\n",
      "iter 10100: 16.281118, duration 17\n",
      "iter 10200: 16.246486, duration 17\n",
      "iter 10300: 16.107612, duration 16\n",
      "iter 10400: 16.119286, duration 17\n",
      "iter 10500: 16.101961, duration 17\n",
      "iter 10600: 16.021105, duration 16\n",
      "iter 10700: 15.855181, duration 17\n",
      "iter 10800: 15.800025, duration 16\n",
      "iter 10900: 15.777610, duration 16\n",
      "iter 11000: 15.626339, duration 16\n",
      "iter 11100: 15.462719, duration 16\n",
      "iter 11200: 15.363876, duration 16\n",
      "iter 11300: 15.258148, duration 16\n",
      "iter 11400: 15.136018, duration 16\n",
      "iter 11500: 15.069384, duration 16\n",
      "iter 11600: 15.022540, duration 16\n",
      "iter 11700: 15.025087, duration 16\n",
      "iter 11800: 14.947074, duration 17\n",
      "iter 11900: 14.832175, duration 16\n",
      "iter 12000: 14.646634, duration 16\n",
      "iter 12100: 14.598675, duration 16\n",
      "iter 12200: 14.546001, duration 16\n",
      "iter 12300: 14.454730, duration 16\n",
      "iter 12400: 14.370805, duration 16\n",
      "iter 12500: 14.324669, duration 16\n",
      "iter 12600: 14.270268, duration 16\n",
      "iter 12700: 14.194618, duration 16\n",
      "iter 12800: 14.106626, duration 16\n",
      "iter 12900: 14.035173, duration 16\n",
      "iter 13000: 13.972545, duration 16\n",
      "iter 13100: 13.866702, duration 16\n",
      "iter 13200: 13.868031, duration 16\n",
      "iter 13300: 13.773661, duration 16\n",
      "iter 13400: 13.684151, duration 16\n",
      "iter 13500: 13.657837, duration 16\n",
      "iter 13600: 13.621514, duration 16\n",
      "iter 13700: 13.613439, duration 16\n",
      "iter 13800: 13.503943, duration 16\n",
      "iter 13900: 13.457793, duration 17\n",
      "iter 14000: 13.363239, duration 17\n",
      "iter 14100: 13.270082, duration 16\n",
      "iter 14200: 13.300356, duration 16\n",
      "iter 14300: 13.283249, duration 17\n",
      "iter 14400: 13.280515, duration 18\n",
      "iter 14500: 13.189012, duration 17\n",
      "iter 14600: 13.202662, duration 17\n",
      "iter 14700: 13.229385, duration 16\n",
      "iter 14800: 13.192011, duration 17\n",
      "iter 14900: 13.095268, duration 17\n",
      "iter 15000: 13.063333, duration 17\n",
      "iter 15100: 13.025528, duration 17\n",
      "iter 15200: 12.943212, duration 16\n",
      "iter 15300: 12.976654, duration 18\n",
      "iter 15400: 12.929570, duration 18\n",
      "iter 15500: 12.886529, duration 17\n",
      "iter 15600: 12.867495, duration 19\n",
      "iter 15700: 12.862635, duration 17\n",
      "iter 15800: 12.831338, duration 18\n",
      "iter 15900: 12.891215, duration 17\n",
      "iter 16000: 12.832670, duration 17\n",
      "iter 16100: 12.824899, duration 16\n",
      "iter 16200: 12.773249, duration 17\n",
      "iter 16300: 12.722324, duration 17\n",
      "iter 16400: 12.687958, duration 17\n",
      "iter 16500: 12.677816, duration 17\n",
      "iter 16600: 12.703363, duration 17\n",
      "iter 16700: 12.647880, duration 17\n",
      "iter 16800: 12.610283, duration 17\n",
      "iter 16900: 12.621305, duration 17\n",
      "iter 17000: 12.603768, duration 16\n",
      "iter 17100: 12.578390, duration 16\n",
      "iter 17200: 12.478358, duration 16\n",
      "iter 17300: 12.418174, duration 16\n",
      "iter 17400: 12.431494, duration 16\n",
      "iter 17500: 12.367493, duration 16\n",
      "iter 17600: 12.303900, duration 16\n",
      "iter 17700: 12.251015, duration 16\n",
      "iter 17800: 12.239787, duration 16\n",
      "iter 17900: 12.176589, duration 16\n",
      "iter 18000: 12.128451, duration 16\n",
      "iter 18100: 12.055500, duration 16\n",
      "iter 18200: 11.957588, duration 16\n",
      "iter 18300: 11.941582, duration 16\n",
      "iter 18400: 11.998437, duration 16\n",
      "iter 18500: 11.933892, duration 16\n",
      "iter 18600: 11.941303, duration 16\n",
      "iter 18700: 11.885236, duration 16\n",
      "iter 18800: 11.911858, duration 16\n",
      "iter 18900: 11.912675, duration 16\n",
      "iter 19000: 11.948032, duration 16\n",
      "iter 19100: 11.945525, duration 16\n",
      "iter 19200: 11.965859, duration 16\n",
      "iter 19300: 12.017580, duration 16\n",
      "iter 19400: 11.917274, duration 16\n",
      "iter 19500: 11.867098, duration 16\n",
      "iter 19600: 11.762583, duration 16\n",
      "iter 19700: 11.689936, duration 17\n",
      "iter 19800: 11.745574, duration 17\n",
      "iter 19900: 11.784608, duration 17\n",
      "iter 20000: 11.799145, duration 17\n",
      "iter 20100: 11.743992, duration 17\n",
      "iter 20200: 11.700320, duration 16\n",
      "iter 20300: 11.633929, duration 16\n",
      "iter 20400: 11.614073, duration 16\n",
      "iter 20500: 11.588584, duration 16\n",
      "iter 20600: 11.593432, duration 17\n",
      "iter 20700: 11.562524, duration 16\n",
      "iter 20800: 11.603497, duration 16\n",
      "iter 20900: 11.578298, duration 17\n",
      "iter 21000: 11.535638, duration 16\n",
      "iter 21100: 11.443762, duration 16\n",
      "iter 21200: 11.437309, duration 16\n",
      "iter 21300: 11.511416, duration 16\n",
      "iter 21400: 11.431161, duration 16\n",
      "iter 21500: 11.420939, duration 16\n",
      "iter 21600: 11.310964, duration 16\n",
      "iter 21700: 11.336428, duration 16\n",
      "iter 21800: 11.233242, duration 16\n",
      "iter 21900: 11.244519, duration 16\n",
      "iter 22000: 11.187573, duration 16\n",
      "iter 22100: 11.188203, duration 16\n",
      "iter 22200: 11.170060, duration 16\n",
      "iter 22300: 11.189821, duration 16\n",
      "iter 22400: 11.166157, duration 16\n",
      "iter 22500: 11.131541, duration 16\n",
      "iter 22600: 11.202706, duration 16\n",
      "iter 22700: 11.204587, duration 16\n",
      "iter 22800: 11.186096, duration 16\n",
      "iter 22900: 11.167442, duration 16\n",
      "iter 23000: 11.197412, duration 16\n",
      "iter 23100: 11.140181, duration 16\n",
      "iter 23200: 11.149885, duration 16\n",
      "iter 23300: 11.077565, duration 16\n",
      "iter 23400: 11.004405, duration 16\n",
      "iter 23500: 11.070211, duration 16\n",
      "iter 23600: 11.076860, duration 16\n",
      "iter 23700: 11.101993, duration 16\n",
      "iter 23800: 11.171190, duration 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23900: 11.149217, duration 16\n",
      "iter 24000: 11.200048, duration 16\n",
      "iter 24100: 11.184291, duration 16\n",
      "iter 24200: 11.222160, duration 16\n",
      "iter 24300: 11.247763, duration 16\n",
      "iter 24400: 11.190674, duration 16\n",
      "iter 24500: 11.128815, duration 16\n",
      "iter 24600: 11.154426, duration 16\n",
      "iter 24700: 11.200064, duration 16\n",
      "iter 24800: 11.156739, duration 16\n",
      "iter 24900: 11.178105, duration 16\n",
      "iter 25000: 11.175456, duration 16\n",
      "iter 25100: 11.175238, duration 16\n",
      "iter 25200: 11.153751, duration 16\n",
      "iter 25300: 11.224236, duration 16\n",
      "iter 25400: 11.187923, duration 16\n",
      "iter 25500: 11.175191, duration 16\n",
      "iter 25600: 11.214223, duration 16\n",
      "iter 25700: 11.229456, duration 16\n",
      "iter 25800: 11.175533, duration 16\n",
      "iter 25900: 11.202877, duration 16\n",
      "iter 26000: 11.131670, duration 16\n",
      "iter 26100: 11.111987, duration 16\n",
      "iter 26200: 11.222385, duration 16\n",
      "iter 26300: 11.237481, duration 16\n",
      "iter 26400: 11.248864, duration 16\n",
      "iter 26500: 11.195802, duration 16\n",
      "iter 26600: 11.160467, duration 16\n",
      "iter 26700: 11.206516, duration 16\n",
      "iter 26800: 11.116749, duration 16\n",
      "iter 26900: 11.116297, duration 16\n",
      "iter 27000: 11.040268, duration 16\n",
      "iter 27100: 11.051296, duration 16\n",
      "iter 27200: 11.047321, duration 16\n",
      "iter 27300: 11.005984, duration 16\n",
      "iter 27400: 11.025116, duration 16\n",
      "iter 27500: 11.000705, duration 16\n",
      "iter 27600: 11.038372, duration 16\n",
      "iter 27700: 10.961929, duration 16\n",
      "iter 27800: 11.065623, duration 16\n",
      "iter 27900: 11.079280, duration 16\n",
      "iter 28000: 11.049607, duration 17\n",
      "iter 28100: 11.036205, duration 17\n",
      "iter 28200: 10.996532, duration 17\n",
      "iter 28300: 10.979652, duration 17\n",
      "iter 28400: 10.984237, duration 16\n",
      "iter 28500: 10.967538, duration 16\n",
      "iter 28600: 10.931247, duration 16\n",
      "iter 28700: 10.893037, duration 16\n",
      "iter 28800: 10.933075, duration 16\n",
      "iter 28900: 10.938234, duration 16\n",
      "iter 29000: 11.020906, duration 16\n",
      "iter 29100: 11.082386, duration 16\n",
      "iter 29200: 11.080879, duration 16\n",
      "iter 29300: 11.039377, duration 16\n",
      "iter 29400: 11.063336, duration 16\n",
      "iter 29500: 11.045738, duration 16\n",
      "iter 29600: 11.016194, duration 16\n",
      "iter 29700: 10.935335, duration 16\n",
      "iter 29800: 11.007448, duration 16\n",
      "iter 29900: 10.994037, duration 16\n",
      "iter 30000: 10.929277, duration 16\n",
      "iter 30100: 11.008700, duration 16\n",
      "iter 30200: 11.004092, duration 16\n",
      "iter 30300: 11.032037, duration 16\n",
      "iter 30400: 11.114695, duration 16\n",
      "iter 30500: 11.066511, duration 16\n",
      "iter 30600: 11.071187, duration 16\n",
      "iter 30700: 11.118583, duration 16\n",
      "iter 30800: 11.019248, duration 16\n",
      "iter 30900: 11.008867, duration 16\n",
      "iter 31000: 11.032826, duration 16\n",
      "iter 31100: 11.036153, duration 16\n",
      "iter 31200: 11.027249, duration 16\n",
      "iter 31300: 11.013257, duration 16\n",
      "iter 31400: 10.946662, duration 16\n",
      "iter 31500: 10.942510, duration 16\n",
      "iter 31600: 11.028076, duration 16\n",
      "iter 31700: 11.060710, duration 16\n",
      "iter 31800: 11.053720, duration 16\n",
      "iter 31900: 11.053539, duration 16\n",
      "iter 32000: 11.092175, duration 16\n",
      "iter 32100: 11.141572, duration 16\n",
      "iter 32200: 11.123584, duration 16\n",
      "iter 32300: 11.161189, duration 16\n",
      "iter 32400: 11.153879, duration 16\n",
      "iter 32500: 11.226568, duration 16\n",
      "iter 32600: 11.271611, duration 16\n",
      "iter 32700: 11.212721, duration 16\n",
      "iter 32800: 11.234039, duration 16\n",
      "iter 32900: 11.212517, duration 16\n",
      "iter 33000: 11.246701, duration 16\n",
      "iter 33100: 11.213351, duration 16\n",
      "iter 33200: 11.160818, duration 16\n",
      "iter 33300: 11.159233, duration 16\n",
      "iter 33400: 11.148181, duration 16\n",
      "iter 33500: 11.155470, duration 16\n",
      "iter 33600: 11.113501, duration 16\n",
      "iter 33700: 11.148388, duration 16\n",
      "iter 33800: 11.255637, duration 16\n",
      "iter 33900: 11.290435, duration 16\n",
      "iter 34000: 11.241156, duration 16\n",
      "iter 34100: 11.120961, duration 16\n",
      "iter 34200: 11.156459, duration 16\n",
      "iter 34300: 11.121047, duration 16\n",
      "iter 34400: 11.094151, duration 16\n",
      "iter 34500: 11.052798, duration 16\n",
      "iter 34600: 11.057712, duration 16\n",
      "iter 34700: 11.165514, duration 16\n",
      "iter 34800: 11.210613, duration 16\n",
      "iter 34900: 11.275020, duration 16\n",
      "iter 35000: 11.211924, duration 16\n",
      "iter 35100: 11.234273, duration 16\n",
      "iter 35200: 11.221152, duration 16\n",
      "iter 35300: 11.197991, duration 16\n",
      "iter 35400: 11.162873, duration 16\n",
      "iter 35500: 11.191456, duration 16\n",
      "iter 35600: 11.246298, duration 16\n",
      "iter 35700: 11.201176, duration 16\n",
      "iter 35800: 11.132279, duration 16\n",
      "iter 35900: 11.115913, duration 16\n",
      "iter 36000: 11.161083, duration 16\n",
      "iter 36100: 11.171925, duration 16\n",
      "iter 36200: 11.233412, duration 16\n",
      "iter 36300: 11.208470, duration 16\n",
      "iter 36400: 11.228333, duration 16\n",
      "iter 36500: 11.164183, duration 16\n",
      "iter 36600: 11.140999, duration 17\n",
      "iter 36700: 11.124674, duration 17\n",
      "iter 36800: 11.059458, duration 17\n",
      "iter 36900: 11.072578, duration 17\n",
      "iter 37000: 11.056380, duration 17\n",
      "iter 37100: 11.077749, duration 17\n",
      "iter 37200: 11.108906, duration 17\n",
      "iter 37300: 11.100172, duration 18\n",
      "iter 37400: 11.136780, duration 17\n",
      "iter 37500: 11.095401, duration 16\n",
      "iter 37600: 11.082525, duration 18\n",
      "iter 37700: 11.087402, duration 17\n",
      "iter 37800: 11.141045, duration 16\n",
      "iter 37900: 11.116443, duration 16\n",
      "iter 38000: 10.975239, duration 16\n",
      "iter 38100: 11.014126, duration 16\n",
      "iter 38200: 10.988202, duration 16\n",
      "iter 38300: 10.970173, duration 16\n",
      "iter 38400: 10.916882, duration 16\n",
      "iter 38500: 10.897062, duration 17\n",
      "iter 38600: 10.958561, duration 16\n",
      "iter 38700: 11.012435, duration 16\n",
      "iter 38800: 11.017073, duration 16\n",
      "iter 38900: 10.971296, duration 16\n",
      "iter 39000: 10.976132, duration 16\n",
      "iter 39100: 10.927911, duration 17\n",
      "iter 39200: 10.869207, duration 17\n",
      "iter 39300: 10.837401, duration 16\n",
      "iter 39400: 10.811880, duration 16\n",
      "iter 39500: 10.856918, duration 16\n",
      "iter 39600: 10.738882, duration 16\n",
      "iter 39700: 10.735582, duration 17\n",
      "iter 39800: 10.755171, duration 17\n",
      "iter 39900: 10.787159, duration 16\n",
      "iter 40000: 10.790214, duration 16\n",
      "sanity check: cost at convergence should be around or below 10\n",
      "training took 6814 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils.treebank import StanfordSentiment\n",
    "\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime = time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "     dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "                                     negSamplingLossAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=100)\n",
    "\n",
    "print(\"sanity check: cost at convergence should be around or below 10\")\n",
    "print(\"training took %d seconds\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b1215",
   "metadata": {},
   "source": [
    "#### Run the following cell to obtain the visulaization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77df26b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEACAYAAACkvpHUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtWElEQVR4nO3daXgUZfb38e9JQggQ9h2CGjERQVEgbMoflU1ABAdFUZxBdB7AkVFwGxwUF0RERdxQBhFHHWRVEAVZ3RWQgIIiEECQVYwbkI0s3M+LNDGEBNJ0J51Ofp/r6qu76l7qVKXhdFXdVWXOOURERAorJNABiIhIcAkLdAAicnosyhoRSUSg4/C7JNLcHrc70GFIwZQ4RIJVJBHcQHKgw/C7GVQKdAhycjpUJSIiXlHiEBERryhxiIiIV5Q4RETEK0ocIiLiFY2qEiltXuBafmco4CjHJi5kPOt4hqPUJIRfacUIerCPcUzESCOTc8giikaM4AD9yCCOcqzjfkYA8DBbqcCbpHMJxh9cxm104Dee40YOcROOcoSyk+v4JzGkMY6JhJBEOs05Sh1q8xj/YCHjeJ5qvM9tLAXgcV6kBgsY6pmWoKE9DpHSZDax/M4ddKYfo+lKJ0bzNWOpwlxG04UqzGMdj+XUP0pV/kU/6vAQu3ids3mFe7iMTM5jBs08tSpSkW95kCuIYBWfcxcAbfiAB+nJaLoSzlbe54acfrOow91cTVP+xi/8G4C6vMVv9AfgayqTQRzXsaJ4Noz4kxKHSGmyl0uIYCGX8DsAbfmDTOK4gXkA3MBcMmmTU78yyygHRLGZEBK5ns1UxBHGFv4gylPrKANYAMC5vE26p/0mzmUM83iUFaTQlyOcm6vfxVTE0Y+tHKU2ALewiizO4lNq8glXE8FCapBVxFtEioASh0jpYhinugHdn+WhpAMQwlHwfM52FFfAoexj/e/mWc5hFKPpTHWewVH+hH6PtTimEnOJpy+HuZ5oZhVmhaTkUeIQKU0a8jmpXMVKqgOwmmqEsYaZ9AFgBn0J4ysvew3hLXoBsIW/UI41nvmRNOQAiYRxiL6F6qkVsznM/wPgOhK8jENKCAvGu+PWqlXLnXXWWYEOQySg0rLSCKl44m+/33/+ncS9iZgZEZUiqNuoLnu27SErM4vQcqFEnRNFePlwdm/dTZXqVahaqyrpaens3LST2BaxAMeVbVy1kZoNanL498OEhoZyxrlnEFYujF9/+pXEvYmElw8nomIEWVlZNIppdFxbgI2rNtKsXbOc+HZ8v4MqNapQs17NfNfraMpRIkJL3y24SoK1a9f+4pyr7Ws/QTmq6qyzziI+Pj7QYYgEVMKOBCJrRRb5cmLqx/DN99/4pa/UlFQ6t+vM4k8WU6VqlXzrJP2SRGx0rF+WJ8czsx/90Y8OVYlIsfj0o0/pGNeRQUMGFZg0JDgE5R6HiBSfrfu3+qWfjpd3ZM33a05dUUo87XGIiIhXlDhERMQrOlQlEqQiwiNI+iUp0GH4XUS4RlSVdEocIkHqjIZnBDoEKaN0qEpERLzil8RhZt3NbIuZbTOzkfmUm5k97ynfYGYtc5VVM7O5ZrbZzDaZWXt/xCQiIkXD58RhZqHAJKAH0BS4wcya5qnWA4jxvAYDL+cqew5Y7JxrAlwIbPI1JhERKTr+2ONoA2xzzv3gnEsHZoLnvjh/6gO84bKtAqqZWX0zqwJ0BF4FcM6lO+f+8ENMIiJSRPyROBoCu3NN7/HMK0yds4FE4DUz+9rMpppZpfwWYmaDzSzezOITExP9ELaIiJwOfyQOy2de3jsnFlQnDGgJvOycawEkAyecIwFwzk1xzsU55+Jq1/b5Hl0iInKa/DEcdw/QKNd0FLCvkHUcsMc5t9ozfy4FJA6R0mrX3l2kpacFOowiEREeoWHDpZA/EscaIMbMooG9QH/gxjx1FgDDzGwm0BY46JzbD2Bmu83sXOfcFqAz8L0fYhIJGmnpacVyl9tAKI0XKIofEodzLtPMhgFLgFBgmnNuo5kN9ZRPBhYBPYFtQAowKFcX/wSmm1k48EOeMhERKWH8cuW4c24R2ckh97zJuT474PYC2n4DxPkjDhERKXq6clxERLyixCFSRrzx6hvMeWvOCfN3/7ibTm07nXa/r0x6hdSUVF9CkyCjxCESpLKysryq/7db/0a/G/v5PY6pL08lNVWJoywJirvjWpQ1IpKcey03q9CMhB0JAYtHQwylqO3+cTcD+g6gRVwLNm7YSPQ50Tz/n+e5rM1l9L+pP598+AmDBg+iWvVqPP3406Snp3Nm9JlMfGkilSIr8fhDj7N00VLCwsLo2Kkjo8eOZsLjE6gUWYmhdwxlw9cbuOv2u6hQoQJt2rfJWW5WVhaPP/Q4Kz9bSXp6OgP/30D+estf+fKzL3lm3DNUr1mdLd9voflFzXlh6gtMmzyNA/sP0O/KflSvWZ25C+cGcKtJcQmKxEEkEdxA8rHJkKUhAR2+qCGGUhy2b93OhEkTaN2uNXf94y5en/o6AOUjyjN/6Xx++/U3/j7g78xaMIuKlSoyaeIkprw4hZsH38wH733Ap2s/xcw4+MfBE/q+6x93MeapMbTv0J4xD4zJmT/jjRlUrlKZRZ8s4siRI1zd7Wou7XQpAN9t+I4PV39Ivfr16NO1D2tWreHW225lyqQpzFk4hxo1axTPhpGAC47EIVIGNYhqQOt2rQHoe31fpk2eBkDvvr0BWPvVWhI2J9CnW/at4TLSM2jVphWVq1SmfER57hl2D52v6EyX7l2O6/fQwUMcPHiQ9h2yb0R9Tf9r+GjZRwB88uEnbPpuEwvfXQjA4UOH2bF9B+XCy3FRq4to0LABAM2aN2P3j7uP21uRskOJQ6SEMrN8pytWqgiAw9Hx8o689NpLJ7Rd+NFCPv/4c959+11em/Iac97/86S4c+6Evv8shMeeeozLulx23OwvP/uS8PDwnOnQkFAyszJPZ7WkFNDJcZESau/uvcSvjgfg3bnv0rp96+PKW7VuxZrVa9ixfQcAqSmpbN+6neSkZA4fOkznKzrzyBOP8P2G42/GULVaVapUqcJXK78CYN7seTlll3a+lDdefYOMjAwg+3BZSnLKSeOMjIwk6bAO35Yl2uMQKaFizo1hzow5jBw+kujG0Qy8dSCv/ee1nPKatWoy8eWJ3H7L7aSnpwNw34P3EVk5klv638KRI0dwzvHQuIdO6PuZl57JOTl+WefLcubfOPBGdu/aTff/645zjhq1ajDtrWknjXPAzQO46ZqbqFOvjk6OlxGWfVF3yWZNLCb3yfELll6wd/HixQGLJ+mXJGKjYwO2fCldEnYknDDYY/ePuxl43UA+XP1hgKLyD/1bKVnMbK1zzuc7dQTtHsfE8ROZN3seDaIaUKNGDZq3aE7lKpWZ/tp00jPSiT47muenPE+FihUYPnQ4ERUi2Jawjb279/LMS88w5605rP1qLS3iWvDs5GcB+GTFJ/kObRQRkT8FZeJISUph0YJFLPl8CVmZWVzxf1fQvEVzelzVgwE3DwBg/KPjmfHGDG4ZegsAB38/yJz357B00VJuvv5m5i+dz9MvPk3Py3ry3YbvaNCwAc899dwJQxtHjBwRyFWVMsrKGa/MfoXtP24PdCgnVb5ceaIaRAU6DClmwZk4DqXQ+/reVKhQAYCuPboCsGXTFp4c8ySHDh4iOTmZSztfmtOma4+umBlNmjahVu1anNfsPABim8SyZ9ce9u/dn+/QRpFAOJJxhEo1S/7ebvKvyaeuJKVOUCaOgoy4bQSvvvUqzS5oxqzps1j52cqcsvDy2UMJQ0JCKF++fM78kJAQMjMzCQkNKXBoo4iI/CkoE0fFyhVZtngZw+4eRlZmFiuWrGDAzQNIOpxE3Xp1ycjIYN7sedSrX6/QfbZq3YpRd49ix/YdRDeOJjUllX1799E4pnERrolI9i1s8t6NIOX3lPwfuFzCpPyeQlKlgofiRoRHFFgmwStoE0fXjl3penFXohpFcWGLC6lcpTL3PnAvvTr1IqpRFE2aNiEpqfBjywsa2qjEIUWtoPueBcNTAZMqatRUWRS0w3Hfnvs2lSIrkZqSSt8efXnyuSe54KILiiUeDTGUopbfEN3CWLpoKQmbExh217Djbmo4fOhwunTvQq+re3HPsHsYPGwwsU18/w7r30JwKfPDce+74z4StiRwJO0I/W7sV2xJQ6Qk69azG916djtpnadffLqYopHSKmgTx6RpkwIdgkixOnar9Tbt27BuzTqant+U6266jgmPT+CXxF94ceqLJGxJYMO6DYydMLbAfq7teS0PPvYgF7a8kPlz5vPChBdwztH5is6MenQUADH1Y7j1tltZvng5ERERvDbzNWrXqV1cqyolnO5VJRJEdv6wM/s/9JXL2bZ1G/PnzGf+0vmMHjuaFya84FVfP+3/ibEPjWX2+7NZ+sVSvln3DYvfz74jQ0pyCi1bt2T5l8tpd0k7pv93elGsjgQpJQ6RINLozEac1+w8QkJCiG0SS4dLO+Rcn7R7126v+lq/bj3tO7SnZq2ahIWF0fe6vqz6YhUA4eHhdO2efX3UBRddwJ5de/y+LhK8guNQVRJpzCDnaqijFY4G9GFKGmIogZL3GqTc1ydlZXr3KNmTDYwJKxeWc+v10NBQMjN1C3X5k18Sh5l1B54DQoGpzrkn8pSbp7wnkALc7Jxbl6s8FIgH9jrneuXt3+1xx/2UiouL00gOER+1iGvB6H+N5rdff6NqtarMnzufW4bcEuiwJAj4nDg8/+lPAroCe4A1ZrbAOZf7IQA9gBjPqy3wsuf9mDuBTUAVX+MRkcKpW68u9z90P/2u7Idzjk7dOnHFlVcEOiwJAj5fx2Fm7YGHnXNXeKbvB3DOjctV5z/Ax865GZ7pLcBlzrn9ZhYFvA6MBe7Kb48jr7i4OBcfH+9T3CIl2elex1HcdB1HcPHXdRz+ODneEMh9KGmPZ15h6zwL3AccPdlCzGywmcWbWXxiYqJPAYuIyOnzR+LI7446eXdj8q1jZr2An51za0+1EOfcFOdcnHMurnZtjScXEQkUfySOPUCjXNNRwL5C1rkE6G1mO4GZQCcz+58fYhIRkSLij8SxBogxs2gzCwf6Awvy1FkA/M2ytQMOOuf2O+fud85FOefO8rT70Dl3kx9iEhGRIuLzqCrnXKaZDQOWkD0cd5pzbqOZDfWUTwYWkT0UdxvZw3EH+bpckdIsv1utl0S6pqlsCoq74+alUVUiIt4rSaOqRESkDFHiEBERryhxiIiIV5Q4RETEK0ocIiLiFSUOERHxihKHiIh4RYlDRES8osQhIiJeUeIQERGvKHGIiIhXlDhERMQrShwiIuIVJQ4REfGKEoeIiHhFiUNERLyixCEiIl5R4hAREa8ocYiIiFeUOERExCtKHCIi4pWwQAcgRWfX3l2kpacFOowSIyI8gjManhHoMESCnl8Sh5l1B54DQoGpzrkn8pSbp7wnkALc7JxbZ2aNgDeAesBRYIpz7jl/xCSQlp5GZK3IQIdRYiT9khToEERKBZ8PVZlZKDAJ6AE0BW4ws6Z5qvUAYjyvwcDLnvmZwN3OufOAdsDt+bQVEZESxB/nONoA25xzPzjn0oGZQJ88dfoAb7hsq4BqZlbfObffObcOwDl3GNgENPRDTCIiUkT8kTgaArtzTe/hxP/8T1nHzM4CWgCr81uImQ02s3gzi09MTPQ1ZhEROU3+SByWzzznTR0ziwTeBoY75w7ltxDn3BTnXJxzLq527dqnHayUXrt/3M282fMCHYZIqeePxLEHaJRrOgrYV9g6ZlaO7KQx3Tn3jh/ikRIgMzOz2Je5e9du5s1R4hApav4YVbUGiDGzaGAv0B+4MU+dBcAwM5sJtAUOOuf2e0ZbvQpscs4944dY5BQmjp/IvNnzaBDVgBo1atC8RXM6XNaBkcNHkpaaxpnRZzJh0gQSf05k+JDhLPx4IZD9a35Q/0EsX7mcDV9v4JF/P0JycjI1atRg4uSJ1K1Xl2t7Xkurtq2IXxVP155dWf7BclrEteDLT7/k4MGDTJg0gbYXt2XW9FkseX8JWVlZbNm0hSHDhpCekc7bM98mPDycN+e+SfUa1dn5w05G3T2KX3/9lQoVKvDUC09xTuw5DB86nMqVK7P+6/Uk/pzIqEdH0evqXjz+0ONsS9hG10u60u+GfgweNjjAW1ukdPJ5j8M5lwkMA5aQfXJ7tnNuo5kNNbOhnmqLgB+AbcArwD888y8B/gp0MrNvPK+evsYk+Vu/bj2LFixiyedLmPq/qaz/ej0Aw4cMZ9Sjo1i+cjlNmjbhmSeeIebcGNIz0vlxx48ALHhnAb3+0ouMjAweuPcBprw5hcWfLub6v17P+EfH5yzj0B+HePuDtxn6z+w/fWZmJgs/XsgjTzzCM0/8+dtgy/dbmPTqJBZ+tJDxY8ZToUIFln6+lFZtWjF3xlwA7rvzPsY8NYbFny7mwcce5P677s9pf+DAAeYvnc/rs19n3EPjAPj3I/+mTfs2LPtimZKGSBHyy3UczrlFZCeH3PMm5/rsgNvzafc5+Z//kCLw1cqvuKLnFVSoUAGArj26kpKcwsGDB2nfoT0A/W7sx5CBQwC46i9X8d689xh21zAWvLOAl197me1bt7Nl0xb69+kPwNGso9SpWydnGb2v6X3cMnv2zv4d0LxFc/b8uCdn/sUdLyayciSRlSOpXKUyXXt0BeC8Zufx/Xffk5yUzNrVa3NiAUg/kp7zufuV3QkJCSG2SSwaLCFSvHTleBmSnb8Lr3ff3gwZOIQeV/XAzDj7nLPZtHETsU1ieW/Fe/m2qVix4nHT4eHhAISGhpKZlXnCfICQkBDKly8PgIUYWZlZHD16lCpVq7Dsi2X5Lie8/J/tvV0vEfGN7lVVhrRp34Zli5eRlpZGclIyK5asoGKlilStVpXVX2aPgn575tu0u6QdAGedfRahIaE8++Sz9O6bvSfROKYxv/3yG/Gr4wHIyMhgy6Ytfo+1cpXKNDqzEe/Ny05Qzjk2frvxpG0iIyNJTkr2eywicjwljjLkolYX0a1HN7pe3JW/D/g7F7a4kMpVKvPs5GcZ88AYurTvwsZvNzLiXyNy2vS+pjfvzHqHq/5yFZC9p/CfN//D4w89TpeLu9Dtkm45ScTfXpz6IjPfmEmXi7tweZvLWbpw6Unrn3f+eYSGhdLl4i5MeXFKkcQkImDBuJsfFxfn4uOL5j+r0iRhR8IJ96pKTkqmUmQlUlNS6dujL08+9yQXXHRBgCIsXkm/JBEbHRvoMEQCxszWOufifO1H5zjKmPvuuI+ELQkcSTtCvxv7lZmkISL+o8RRxkyaNinQIYhIkNM5DhER8YoSh4iIeEWHqkqxiPAIPbwol4jwiECHIFIqKHGUYnpMqogUBR2qEhERryhxiIiIV5Q4RETEK0ocIiLiFSUOERHxihKHiIh4RYlDRES8osQhIiJeUeIQERGvKHGIiIhXlDhERMQrfkkcZtbdzLaY2TYzG5lPuZnZ857yDWbWsrBtRUSkZPE5cZhZKDAJ6AE0BW4ws6Z5qvUAYjyvwcDLXrQVEZESxB97HG2Abc65H5xz6cBMoE+eOn2AN1y2VUA1M6tfyLYiIlKC+CNxNAR255re45lXmDqFaSsiIiWIPxKH5TPPFbJOYdpmd2A22MzizSw+MTHRyxBFRMRf/JE49gCNck1HAfsKWacwbQFwzk1xzsU55+Jq167tc9AiInJ6/JE41gAxZhZtZuFAf2BBnjoLgL95Rle1Aw465/YXsq2IiJQgPj861jmXaWbDgCVAKDDNObfRzIZ6yicDi4CewDYgBRh0sra+xiQiIkXHnMv3lEKJFhcX5+Lj4wMdhohIUDGztc65OF/70ZXjIiLiFSUOERHxihKHiIh4RYlDRES8osQhIiJeUeIQERGvKHGIiIhXlDhERMQrShwiIuIVJQ4REfGKEoeIiHhFiUNERLyixCEiIl5R4hAREa8ocYiIiFd8fpCTiIiUTLv27iItPe3PGeUpb00s5rQ7TCLN7XG7lThEREqptPQ0ImtF/jkjgqPcQPJpdziDSqA9DgmgE34NBUBEeARnNDwjoDGIBBslDgmYE34NBUDSL0kBXb5IMNLJcRGRsm4M7wKwkCge5cNTVVfiEBEp6x6kjzfVdahKRKQseYbBJNEfgEje4i6m8jBbeZhCj7byaY/DzGqY2TIz2+p5r15Ave5mtsXMtpnZyFzznzKzzWa2wczmmVk1X+KR0ueVSa+QmpLqt3oiZVomFUjieq7jSq6lF0kM4C3O97YbXw9VjQRWOOdigBWe6eOYWSgwCegBNAVuMLOmnuJlwPnOueZAAnC/j/FIKTP15amkpp46IRS2nkiZlkklKvABTUilKSlUYBEHaONtN74equoDXOb5/DrwMfCvPHXaANuccz8AmNlMT7vvnXNLc9VbBVzrYzwSxFKSUxgycAj79+3naNZRel3diwP7D9Dvyn5Ur1mduQvnMnLESNavW09aahpX9rmSe0bdw6svv3pCvU9WfMLTjz9Neno6Z0afycSXJlIpslKgV1GkVPA1cdR1zu0HcM7tN7M6+dRpCOzONb0HaJtPvVuAWQUtyMwGA4MBzjhD4+5Lo4+Wf0S9+vV4c+6bABw6eIhZ02cxZ+EcatSsAcC/HvwX1WtUJysri+uvup7vv/ueW2+7lSmTpuTU++3X33juqeeYtWAWFStVZNLESUx5cQojRo4I5OqJBF4YyaTSna28SBZGKj04hzs46G03p2Bmy4F6+RSNKuQyLJ95Ls8yRgGZwPSCOnHOTQGmAMTFxbmC6knwatK0CWMeGMPY0WPp0r0LbS8+8ffFe/PeY/p/p5OVmcWBnw6wdfNWmp7f9Lg6a79aS8LmBPp0yx4okpGeQas2rYplHURKtDBSiWQeM1gEZJ8cv5HveNjbbk7BOdeloDIzO2Bm9T17G/WBn/OptgdolGs6CtiXq4+BQC+gs3NOCaEMaxzTmA8++YAPl37IuIfHcWmnS48r37VzF/95/j8s/Hgh1apXY/jQ4aQdOfHKc4ej4+Udeem1l4ordJHgcRc5P8JzHBtRdSV7uJJOp+rC15PjC4CBns8DwXMRyfHWADFmFm1m4UB/TzvMrDvZ50R6O+dSfIxFgtxP+3+iQsUKXNP/GobeMZRv139LZGQkSYezr+4+fPgwFSpVoErVKiT+nMhHyz7KaZu7XqvWrVizeg07tu8AIDUlle1btxf/ComUUr6e43gCmG1mtwK7gH4AZtYAmOqc6+mcyzSzYcASIBSY5pzb6Gn/IlAeWGZmAKucc0N9jEmC1OaNm3nswcewEKNcWDnGTRzH2q/WctM1N1GnXh3mLpzL+c3P5/I2l3PGWWfQul3rnLYDbh5wXL2JL0/k9ltuJz09HYD7HryPxjGNA7VqIqWKBePRobi4OBcfHx/oMMRHCTsSSsS9qmKjYwMag0hRyftvrGGjhhsYQY/T7nAGldxmt1W3HBEREa8ocYiIiFd0ryoRkVIqIjzi+EcHpBFy7GFMpyWJNFDiEBEptU54SNkRjrjNbquv/SpxSMCc8GsoQDGIiHeUOCRg9MhWkeCkk+MiIuIVJQ4REfGKEoeIiHhFiUNERLyixCEiIl5R4hAREa8ocYiIiFeUOERExCtKHCIi4hUlDhER8YoSh4iIeEWJQ0REvKLEISIiXlHiEBERryhxiIiIV/Q8DhE5wa69u0hLTwt0GF6JCI/QM16KiU+Jw8xqALOAs4CdwHXOud/zqdcdeA4IBaY6557IU34P8BRQ2zn3iy8xiYjv0tLTiKwVGegwvBLop0mWJb4eqhoJrHDOxQArPNPHMbNQYBLQA2gK3GBmTXOVNwK6Art8jEVERIqBr4mjD/C65/PrwNX51GkDbHPO/eCcSwdmetodMxG4D3A+xiIiIsXA18RR1zm3H8DzXiefOg2B3bmm93jmYWa9gb3OufWnWpCZDTazeDOLT0xM9DFsERE5XadMHGa23My+y+fV51Rtj3WRzzxnZhWBUcDownTinJvinItzzsXVrl27kIsWkZJm6aKlvPjMi6fV9vmnn/dzNHI6Tnly3DnXpaAyMztgZvWdc/vNrD7wcz7V9gCNck1HAfuAxkA0sN7Mjs1fZ2ZtnHM/ebEOIhJEuvXsRree3U6r7QsTXuCOe+7wc0TiLV+H4y4ABgJPeN7fzafOGiDGzKKBvUB/4Ebn3EZyHdoys51AnEZViZRct9xwC/v27uNI2hFuve1Wbhp0EzPemMGkiZOoV78e0Y2jCQ8PZ+yEsSz9YCnPP/k86RnpVK9RnRenvkjtOrWZNX0WG9ZtYOyEsQwfOpzKlSuz/uv1JP6cyKhHR9Hr6l4c+OkAt918G4cPHyYrM4txE8exYskK0lLT6HpJV85tci4vvnp6ey3iO18TxxPAbDO7lexRUf0AzKwB2cNuezrnMs1sGLCE7OG40zxJQ0SCzIRJE6heozqpqalcedmVdL6iM88++SyLP11MZOVIrut1HU3Pzx402aZdG9778D3MjLdef4uXnn2Jhx5/6IQ+Dxw4wPyl89mWsI1B1w+i19W9mDdnHpd2vpQ7772TrKwsUlNSaXtxW16b8hrLvlhW3KstefiUOJxzvwKd85m/D+iZa3oRsOgUfZ3lSywiUvSmTZ7GB+9/AMC+vft4e+bbtLukHdVrVAeg19W9+GHbDwDs37ef226+jZ8P/Ex6ejpnnJn/xXndr+xOSEgIsU1iOTbw5aKWF3H3P+4mMyOTK3pdwfnNzy+GtZPC0i1HRKRQvvzsSz77+DPeW/4ey79czvnNz6dxTOMC6z9474MMGjKIFatWMP658Rw5ciTfeuHlw3M+O5c9Kr/dJe14e/Hb1GtQjzsH38mct+b4d2XEJ0ocIlIohw8dpmq1qlSoWIFtCdtYt2YdqSmprPpiFX/8/geZmZksWvDngYVDhw5Rr349AK//49+zaw+1atdiwM0D6P/X/ny7/lsAypUrR0ZGhv9WSk6L7lUlIoVyWZfLePPVN+nSvgtnx5xNy9YtqdegHv+8+5/06tSLevXrEdMkhspVKwNw9/13M2TgEOrVr0fL1i3Z/ePuUyzhT19+9iWTn59MWLkwKlWqxHP/eQ6AATcPoEv7Llxw4QU6OR5AdmzXMJjExcW5+Pj4QIchUmol7Ego9L2qkpOSqRRZiczMTG698Vb6/7U/Pa7qUcQRnijplyRio2OLfbnBxMzWOufifO1Hexwi4pMJ4ybw2cefcSTtCJd2upTuvboHOiQpYkocIuKT0WMLdfMHKUV0clxERLyixCEiIl7RoSoROUFEeETQPRgpIjwi0CGUGUocInICPYJVTkaHqkRExCulfo9j195dpKWnBTqMQokIj9AvPREp8Up94khLTyv0hUyBFmzHlEWkbNKhKhER8YoSh4iIeEWJQ0REvKLEISIiXlHiEBERr5T6UVV5pSSnMGTgEPbv28/RrKPced+dRJ8dzSP/foTk5GRq1KjBxMkTqVuvLtP/O53pr00nPSOd6LOjeX7K81SoWIH35r3HxCcmEhIaQpUqVXhn8TukpaVx/4j72fD1BkLDQnno8Ye4pOMlzJo+i2WLlpGaksrOHTvpcVUPHhjzQKA3g4jIaStzieOj5R9Rr3493pz7JgCHDh7ipmtu4rWZr1GzVk3efftdxj86nmdeeoYeV/VgwM0DABj/6HhmvDGDW4bewrPjn2X6vOnUb1Cfg38cBOC/r/wXgBWrVrAtYRs3XH0Dn637DICN325kyWdLCC8fTsdWHRk0ZBANoxoW/8qLiPhBmUscTZo2YcwDYxg7eixduneharWqbNm0hf59+gNwNOsoderWAWDLpi08OeZJDh08RHJyMpd2vhSAuHZxjLhtBFf95aqcB9asWbmGQUMGAXBO7DlENYrih20/ANDh0g5UqVoFgNhzY9m7e68Sh4gErTKXOBrHNOaDTz7gw6UfMu7hcXS8vCOxTWJ5b8V7J9QdcdsIXn3rVZpd0IxZ02ex8rOVAIx/djzr1qxjxZIVdOvQjaWfL+VkT1IMDw/P+RwSGkJmZqb/V0xEpJj4dHLczGqY2TIz2+p5r15Ave5mtsXMtpnZyDxl//SUbTSzJ32JpzB+2v8TFSpW4Jr+1zD0jqF8Hf81v/3yG/Grsx9Fm5GRwZZNWwBIOpxE3Xp1ycjIYN7seTl97PxhJy1bt+TeB+6lRs0a7Nu7j7aXtM2ps33rdvbu2UvjmMZFvToiIsXO1z2OkcAK59wTnoQwEvhX7gpmFgpMAroCe4A1ZrbAOfe9mV0O9AGaO+eOmFkdH+M5pc0bN/PYg49hIUa5sHKMmziO0LBQRt83mkOHDpGVmcXf//F3zj3vXO594F56depFVKMomjRtQlJS9i1BHnvwMXZs34Fzjg6XdqDZBc04J/YcRg4fSed2nQkNC2XiyxMpX758Ua+OiEixs5MdYjllY7MtwGXOuf1mVh/42Dl3bp467YGHnXNXeKbvB3DOjTOz2cAU59xyb5YbFxfn4uPjC1U3YUdCUN2rKjY6NtBhiEgpZWZrnXNxvvbj63UcdZ1z+wE87/ntMTQEduea3uOZBxAL/J+ZrTazT8ysdUELMrPBZhZvZvGJiYk+hi0iIqfrlIeqzGw5UC+folGFXIblM+/Ybk4YUB1oB7QGZpvZ2S6f3SDn3BRgCmTvcRRy2SIi4menTBzOuS4FlZnZATOrn+tQ1c/5VNsDNMo1HQXsy1X2jidRfGVmR4FagHYpRERKKF8PVS0ABno+DwTezafOGiDGzKLNLBzo72kHMB/oBGBmsUA48IuPMYmISBHydVTVE2QfXroV2AX0AzCzBsBU51xP51ymmQ0DlgChwDTn3EZP+2nANDP7DkgHBuZ3mMoXEeERQfOApIjwiECHICJySj6NqgoUb0ZViYhItpIyqkpERMoYJQ4REfGKEoeIiHilzN3kUCSvXXt3kZaeFugwckSER3BGwzMCHYZIgZQ4pMxLS08rUbelCZZRgFJ26VCViIh4RYlDRES8osQhIiJeUeIQKULX9ryW9evWnzB/1vRZjLq7sPcJFSlZlDhEikhWVlagQxApEkocIvl46dmXePXlVwF4aORD9OvVD4DPPv6Mf/79n8yfM5/O7TrTqW0nxo4em9Mupn4MTz32FL0u78Xar9Ye1+es/82iQ4sOXNPjGuJX6ZY5EryUOETy0fbitqxeuRqADV9vICUphYyMDNasXEN042jGPjSW2e/PZukXS/lm3Tcsfn8xACnJKZzb9Fze/+h92rRvk9PfgZ8O8PTjT/PusneZ8e4MEjYnBGS9RPxBiUMkH81bNOfbb74l6XAS4eXDadWmFevXrWf1ytVUqVqF9h3aU7NWTcLCwuh7XV9WfbEKgNDQUK7sc+UJ/X0d/3VOm/DwcHr37V3cqyTiN0ocIvkoV64cUWdEMet/s4hrE0ebi9vw5Wdf8uOOH2nYqGGB7cpHlCc0NDTfMrP8HoYpEnyUOEQK0O7idkx+YTJtL2lL24vb8ua0N2l2QTNatm7Jqi9W8duvv5GVlcX8ufNp36H9SftqEdeClZ+v5LdffyMjI4P3579fTGsh4n9B+TwOM0sEfvSiSS2C88mCwRh38MVcnvKUpxrGb8fNzyCSFM6mCt9hHOUQTQjnFyL4hXSqcYQ6AIRxmArsB+Ag51OV73L6SKIxEewjjFSOUJ0j1CWEDEJIBYyK7D0hnjRCOMKRQkYffNs7OGOG0hH3mc652r52GJSJw1tmFu+Ph5cUt2CMOyhjbmIx/MRcRtAj0LEAMINKbrPbWpiqQbm9gzBmUNy56VCViIh4RYlDRES8UlYSx5RAB3CagjHuYIwZKvO/QIdwmoJxewdjzKC4c5SJcxwiJ2NNLIYbSA50HDm8OMchEgh6kJNIEmnMoFKgw8iRRMl5HKFIPrTHISIiXgnqcxxmVsPMlpnZVs979QLqdTezLWa2zcxG5po/y8y+8bx2mtk3nvlnmVlqrrLJJSjmh81sb67YeuYqu99Tf4uZXeGvmP0U91NmttnMNpjZPDOr5pnv921dUAy5ys3MnveUbzCzloWIv1DrH4i4zayRmX1kZpvMbKOZ3ZmrTYHfl0DH7SnbaWbfemKLzzW/SLe3D9v63Fzb8hszO2Rmwz1lJWFbNzGzlWZ2xMzuKUzb09rWzrmgfQFPAiM9n0cC4/OpEwpsB84GwoH1QNN86k0ARns+nwV8VxJjBh4G7smnTVNPvfJAtKd9aAmKuxsQ5vk8/lh7f2/rwvy9gZ7AB4AB7YDVhYj/lOsfwLjrAy09nysDCaf6vpSEuD1lO4Fap/N9C1TMefr5ieyL6krKtq4DtAbG5o7F39/toN7jAPoAr3s+vw5cnU+dNsA259wPzrl0YKanXQ4zM+A6YEbRhZrDLzEX0O9M59wR59wOYJunH3/xKW7n3FLnXKan3iogyo+xFSqGXPoAb7hsq4BqZlb/FG0Ls/4Bids5t985tw7AOXcY2AQUfEOtEhL3Kfotyu3tr5g7A9udc97cxcIXp4zbOfezc24NkOFFW6+3dbAnjrrOuf0Anvc6+dRpCOzONb2HE/9R/R9wwLnjRrJEm9nXZvaJmf1fCYt5mGf3eVqu3crCrKcv/LWtAW4h+9fcMf7c1oWJoaA6J2tbmPX3hS9x5zCzs4AWwOpcs/P7vviLr3E7YKmZrTWzwbnqFOX29su2Bvpz4o/NQG/r02nr9bYu8YnDzJab2Xf5vE71Czyni3zm5R0RcAPHfwH2A2c451oAdwFvmVmVEhLzy0Bj4CJPnBMK0aYkxH1sGaOATGC6Z5ZP2/p0YjhJHZ+3oQ98iTu70CwSeBsY7pw75Jld0PfFX3yN+xLnXEugB3C7mXX0Z3AF8Me2Dgd6A3NylZeEbV0UbU9Q4ofjOue6FFRmZgeO7ap7diN/zqfaHqBRrukoYF+uPsKAvkCrXMs8Atk3mXPOrTWz7UAsUKjHthVlzM65A7n6egV4/1RtCqsYtvVAoBfQ2XkOqPq6rb2N4RR1wk/StjDr7wtf4sbMypGdNKY75945VuEk3xd/8Slu59yx95/NbB7Zh1Q+pWi3t08xe/QA1uXeviVkW59OW6+3dYnf4ziFBcBAz+eBwLv51FkDxJhZtOdXQn9Pu2O6AJudc3uOzTCz2mYW6vl8NhAD/FASYs5znPUvkHMn1gVAfzMrb2bRnpi/8lPM/oi7O/AvoLdzLuVYgyLY1qf6ex9bl79ZtnbAQc8u+snaFmb9fXHacXvO0b0KbHLOPZO7wUm+LyUh7kpmVtkTZyWyB1Dk/j4X1fb25TtyTN6jFCVlW59OW++3tT/O9gfqBdQEVgBbPe81PPMbAIty1etJ9kiT7cCoPH38FxiaZ941wEayRx6sA64qKTEDbwLfAhs8f/D6ucpGeepvAXqUpG1N9sn63cA3ntfkotrW+cUADD32dyZ7t32Sp/xbIK4Q8ee7/n7exqcVN9CB7MMOG3Jt356n+r6UgLjP9vzd13u+A8W2vX38jlQEfgWq5umzJGzremTvXRwC/vB8ruLv77YuABQREa8E+6EqEREpZkocIiLiFSUOERHxihKHiIh4RYlDRES8osQhIiJeUeIQERGv/H8ctlnTf5zNegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords, :], wordVectors[nWords:, :]),\n",
    "    axis=0)\n",
    "\n",
    "visualizeWords = [\n",
    "    'state', 'season', 'company', 'world', 'against', \n",
    "    'president', 'game', 'million', 'oil', 'government'\n",
    "]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U, S, V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:, 0:2])\n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i, 0], coord[i, 1], visualizeWords[i],\n",
    "             bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:, 0]), np.max(coord[:, 0])))\n",
    "plt.ylim((np.min(coord[:, 1]), np.max(coord[:, 1])))\n",
    "\n",
    "plt.savefig('word_vectors.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0185d3",
   "metadata": {},
   "source": [
    "#### Run the following cell to obtain the k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2c62c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: \"state\" is close to ['state', 'organization', 'wholesalers', 'thoroughbred', 'tasmania', 'filter', 'limit', 'reefs', 'trench', 'deserve']\n",
      "Word: \"season\" is close to ['season', 'modestly', 'declared', 'celebrating', 'cities', 'spain', 'uncut', 'compiled', 'triumphant', 'tomjanovich']\n",
      "Word: \"company\" is close to ['company', 'servisair', 'agenda', 'respected', 'kenyon', 'octroi', 'oct', 'playoff', 'persistance', 'depleted']\n",
      "Word: \"world\" is close to ['world', 'correspondent', 'retailer', 'data', 'roddick', 'footwear', 'feed', 'mph', 'andy', 'enemy']\n",
      "Word: \"against\" is close to ['against', 'conte', 'underlying', 'behemoth', 'transmitted', 'damascus', 'sonyis', 'philip', 'direction', 'hordes']\n",
      "Word: \"president\" is close to ['president', 'is', 'new', 'denies', 'lake', 'paralympic', 'undisputed', 'wakefield', 'swimmers', 'completes']\n",
      "Word: \"game\" is close to ['game', 'defend', 'trusted', 'ingushetia', 'organizer', 'milosevic', 'barneys', 'koskie', 'grosjean', 'play']\n",
      "Word: \"million\" is close to ['million', 'breed', 'stranded', 'cash', 'buy', 'trades', 'information', 'adverse', 'mikael', 'songs']\n",
      "Word: \"oil\" is close to ['oil', 'veolia', 'tomcat', 'china', 'minisd', 'out', 'championship', 'changed', 'available', 'xeni']\n",
      "Word: \"government\" is close to ['government', 'altria', 'championship', 'travelers', 'die', 'erasing', 'available', 'reads', 'adewale', 'caivano']\n"
     ]
    }
   ],
   "source": [
    "centerVectors = wordVectors[:nWords, :]\n",
    "outputVectors = wordVectors[nWords:, :]\n",
    "for word in visualizeWords:\n",
    "    idx = tokens[word]\n",
    "    vec = outputVectors[idx]\n",
    "    indices = knn(vec, outputVectors, 10)\n",
    "    closed_words = [list(tokens.keys())[i] for i in indices]\n",
    "    print('Word: \"{}\" is close to {}'.format(word, closed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf28dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
